# Week 2: PM Discovery + AI Project Workflows

## üéØ Week Overview

**Focus Areas:**
- Opportunity Discovery systems (Opportunity Solution Trees, JTBD)
- ML project lifecycle and workflows
- How to choose and prioritize AI projects
- Understanding data requirements for AI products

**Deliverables:**
- "How to choose AI projects" 1-pager
- Opportunity Solution Tree for one AI opportunity
- ML Project Lifecycle document

## üß† Deep Dive Resources

- [AI Discovery Experiments Deep Dive](./ai-discovery-experiments-deep-dive.md) ‚Äî assumption mapping, experiment toolkit, and continuous discovery loop guidance for AI projects.

---

## üìö Learning Objectives

By the end of this week, you will:

1. ‚úÖ Master discovery frameworks (OST, JTBD deep dive)
2. ‚úÖ Understand the complete ML project lifecycle
3. ‚úÖ Know how to evaluate AI project feasibility
4. ‚úÖ Identify data requirements and constraints
5. ‚úÖ Build a systematic approach to AI opportunity selection

---

## üóìÔ∏è Daily Breakdown

### **Day 1 (Monday): Opportunity Solution Trees (OST)**

**Morning Session (2 hours)**
- **Topic:** Introduction to Opportunity Solution Trees
- **Learning:**
  - What are OSTs and why they matter for PMs
  - How to build an OST: Outcome ‚Üí Opportunities ‚Üí Solutions ‚Üí Experiments
  - Continuous discovery mindset
- **Resources:**
  - üìö [Teresa Torres - Continuous Discovery Habits](https://www.producttalk.org/opportunity-solution-tree/) (Read: Chapter on OST)
  - üé• [How to Create an Opportunity Solution Tree](https://www.youtube.com/watch?v=l6vQ5Oc3y0A) - Teresa Torres
  - üìñ [OST Template & Guide](https://www.producttalk.org/2023/08/opportunity-solution-trees/)

**Evening Session (2 hours)**
- **Activity:** Build Your First OST
- **Exercise:**
  - Pick one AI opportunity from your Week 1 list
  - Define the outcome (what changes for users?)
  - Map 5+ opportunities (user needs/pain points)
  - For each opportunity, brainstorm 2-3 solution approaches
  - Identify experiments to test assumptions
- **Deliverable Start:** Begin OST document for your selected opportunity

**Interview Prep:**
- Practice explaining: "How do you discover and validate opportunities?"
- Framework: "I use Opportunity Solution Trees to connect business outcomes to user needs, then identify multiple solution paths and run experiments to validate assumptions."

---

### **Day 2 (Tuesday): Jobs To Be Done - Deep Dive**

**Morning Session (2 hours)**
- **Topic:** Advanced JTBD Framework
- **Learning:**
  - Functional, emotional, and social jobs
  - Forces of progress (push, pull, anxiety, habit)
  - Switch interviews and job stories
  - JTBD vs. personas
- **Resources:**
  - üìö [Competing Against Luck](https://www.christenseninstitute.org/jobs-to-be-done/) - Key chapters
  - üé• [Bob Moesta - JTBD Masterclass](https://www.youtube.com/results?search_query=bob+moesta+jobs+to+be+done)
  - üìñ [Intercom's JTBD Guide](https://www.intercom.com/blog/using-job-stories-design-features-ui-ux/)

**Evening Session (2 hours)**
- **Activity:** JTBD Deep Dive Exercise
- **Practice:**
  - Pick 3 successful AI products (e.g., Notion AI, Midjourney, Perplexity)
  - For each, identify:
    - What's the functional job?
    - What's the emotional job?
    - What's the social job?
    - What are the forces of progress?
  - Write job stories: "When [situation], I want to [motivation], so I can [outcome]"
- **Deliverable:** JTBD analysis document with 3 AI product examples

**Interview Prep:**
- Common question: "How do you understand user needs?"
- Answer with JTBD framework + example from your analysis

---

### **Day 3 (Wednesday): Discovery Interviews & User Research**

**Morning Session (2 hours)**
- **Topic:** Conducting Generative Discovery Interviews
- **Learning:**
  - Interview techniques for uncovering real needs
  - Open-ended questions vs. leading questions
  - Listening for struggles, not solutions
  - Synthesizing interview insights
- **Resources:**
  - üìö [The Mom Test](https://www.momtestbook.com/) - Rob Fitzpatrick (Read key chapters)
  - üé• [How to Do User Interviews](https://www.youtube.com/watch?v=qAws7eXItMk) - YC Startup School
  - üìñ [Teresa Torres - Discovery Interviews](https://www.producttalk.org/2022/03/discovery-interviews/)

**Evening Session (2 hours)**
- **Activity:** Create Interview Guide
- **Exercise:**
  - For your selected AI opportunity, create:
    - 10 open-ended discovery questions
    - Follow-up probes
    - Success criteria for insights
  - Practice interviewing a friend/colleague
  - Document what you learned
- **Deliverable:** User Research Interview Guide + 1 interview summary

**Interview Prep:**
- "How do you validate product ideas?"
- "Tell me about a time you discovered an unexpected user need"

---

### **Day 4 (Thursday): ML Project Lifecycle**

**Morning Session (2 hours)**
- **Topic:** Understanding the ML Project Lifecycle
- **Learning:**
  - Phase 1: Problem Definition & Data Assessment
  - Phase 2: Data Collection & Preparation
  - Phase 3: Model Training & Evaluation
  - Phase 4: Deployment & Monitoring
  - Phase 5: Iteration & Improvement
  - Common failure modes at each stage
- **Resources:**
  - üìö [Google's ML Workflow](https://cloud.google.com/architecture/mlops-continuous-delivery-and-automation-pipelines-in-machine-learning)
  - üé• [Andrew Ng - ML Project Lifecycle](https://www.deeplearning.ai/courses/machine-learning-engineering-for-production-mlops/)
  - üìñ [Full Stack Deep Learning - ML Projects](https://fullstackdeeplearning.com/course/2022/)
  - üìñ [Chip Huyen - Designing ML Systems](https://huyenchip.com/machine-learning-systems-design/toc.html)

**Evening Session (2 hours)**
- **Activity:** Map ML Lifecycle for Real Products
- **Exercise:**
  - Study 2 AI products (e.g., Spotify Discover Weekly, YouTube recommendations)
  - For each, map out:
    - What data do they collect?
    - How is the model trained?
    - How is it deployed?
    - How do they monitor performance?
    - How do they handle drift?
- **Deliverable Start:** ML Project Lifecycle document template

**Interview Prep:**
- "Walk me through how you would build a recommendation system"
- Use ML lifecycle framework in your answer

---

### **Day 5 (Friday): How to Choose AI Projects**

**Morning Session (2 hours)**
- **Topic:** AI Project Selection Framework
- **Learning:**
  - Impact vs. Feasibility matrix for AI
  - Data availability assessment
  - Technical complexity evaluation
  - Time-to-value estimation
  - Cost modeling (compute, data, talent)
  - Risk assessment (ethical, technical, business)
- **Resources:**
  - üìö [AI Project Canvas](https://www.oreilly.com/library/view/practical-deep-learning/9781492034858/) - Chapter on project selection
  - üìñ [Andrew Ng - AI Transformation Playbook](https://landing.ai/resources/ai-transformation-playbook/)
  - üìñ [a16z - How to Choose AI Projects](https://a16z.com/2016/06/10/ai-deep-learning-startup/)

**Evening Session (2 hours)**
- **Activity:** Build AI Project Selection Framework
- **Create:** "How to Choose AI Projects" 1-pager with:
  1. **Impact Assessment:**
     - User value potential (high/medium/low)
     - Business impact ($, growth, retention)
     - Competitive advantage
  2. **Feasibility Assessment:**
     - Data availability (scale: 1-5)
     - Technical complexity (scale: 1-5)
     - Team capability
     - Time to MVP
  3. **Risk Assessment:**
     - Ethical considerations
     - Regulatory concerns
     - Technical risk
     - Business risk
  4. **Decision Matrix:**
     - Quick wins (high impact, high feasibility)
     - Strategic bets (high impact, low feasibility)
     - Fill-ins (low impact, high feasibility)
     - Don't do (low impact, low feasibility)
  5. **Prioritization Framework:**
     - RICE scoring for AI projects
     - Modified for AI-specific factors

**Deliverable:** Complete "How to Choose AI Projects" 1-pager

**Interview Prep:**
- "How do you prioritize which AI features to build?"
- "What factors do you consider when choosing an ML project?"

---

### **Day 6-7 (Weekend): Apply Framework & Case Studies**

**Saturday (3 hours)**
- **Activity:** Apply Your Framework
- **Exercise:**
  - Take your 10 AI opportunities from Week 1
  - Score each using your selection framework
  - Rank them by priority
  - Select top 3 for potential portfolio projects
- **Deep Dive Study:**
  - Read case studies of AI project failures
  - Identify what went wrong (data, scope, value, etc.)
  - Document lessons learned
- **Resources:**
  - üìñ [Why AI Projects Fail](https://sloanreview.mit.edu/article/why-ai-projects-fail/)
  - üìñ [Google's ML Rules](https://developers.google.com/machine-learning/guides/rules-of-ml)

**Sunday (2 hours)**
- **Activity:** Polish Deliverables
  - Review and refine OST document
  - Enhance "How to Choose AI Projects" 1-pager
  - Complete ML Project Lifecycle document
  - Add real-world examples to each framework
- **Reflection:**
  - Write: What makes a good AI project vs. a bad one?
  - Document: Top 3 insights from this week
  - Prepare: Questions for Week 3 on metrics

---

## üìù Deliverables Checklist

### Required Deliverables:

- [ ] **Opportunity Solution Tree** (`opportunity-solution-tree.md`)
  - Clear outcome definition
  - 5+ mapped opportunities
  - 2-3 solutions per opportunity
  - Experiment hypotheses
  - Visual diagram (can use Miro, FigJam, or markdown)

- [ ] **How to Choose AI Projects - 1-Pager** (`ai-project-selection.md`)
  - Impact assessment criteria
  - Feasibility assessment framework
  - Risk evaluation checklist
  - Decision matrix
  - RICE scoring template for AI
  - Applied to 3 example projects

- [ ] **ML Project Lifecycle Document** (`ml-project-lifecycle.md`)
  - All 5 phases mapped
  - PM responsibilities at each phase
  - Common failure modes
  - 2 real-world examples analyzed

### Optional Deliverables:

- [ ] **JTBD Analysis Document** (`jtbd-analysis.md`)
  - 3 AI product analyses
  - Job stories
  - Forces of progress

- [ ] **User Research Interview Guide** (`interview-guide.md`)
  - 10 discovery questions
  - Follow-up probes
  - 1 interview summary

---

## üìñ Recommended Resources (Comprehensive List)

### Discovery Frameworks:
- **Books:**
  - üìö *Continuous Discovery Habits* - Teresa Torres ‚≠ê MUST READ
  - üìö *The Mom Test* - Rob Fitzpatrick ‚≠ê MUST READ
  - üìö *Competing Against Luck* - Clayton Christensen (JTBD)

- **Articles & Guides:**
  - [Product Talk - Teresa Torres Blog](https://www.producttalk.org/blog/)
  - [Intercom on JTBD](https://www.intercom.com/blog/jobs-to-be-done/)
  - [JTBD Playbook](https://jobs-to-be-done.com/the-jobs-to-be-done-playbook-5f75c61fdba)

- **Videos:**
  - [Teresa Torres - OST Workshop](https://www.youtube.com/c/ProductTalk)
  - [Bob Moesta - Switch Interviews](https://www.youtube.com/results?search_query=bob+moesta+switch+interview)

### ML Project Lifecycle:
- **Courses:**
  - üéì [Andrew Ng - MLOps Specialization](https://www.deeplearning.ai/courses/machine-learning-engineering-for-production-mlops/) ‚≠ê
  - üéì [Full Stack Deep Learning](https://fullstackdeeplearning.com/) ‚≠ê
  - üéì [Google Cloud - ML Engineering](https://www.cloudskillsboost.google/paths/17)

- **Books & Guides:**
  - üìö *Designing Machine Learning Systems* - Chip Huyen ‚≠ê MUST READ
  - üìö *Building Machine Learning Powered Applications* - Emmanuel Ameisen
  - üìñ [Google's Rules of ML](https://developers.google.com/machine-learning/guides/rules-of-ml) ‚≠ê
  - üìñ [AWS ML Lens](https://docs.aws.amazon.com/wellarchitected/latest/machine-learning-lens/machine-learning-lens.html)

- **Articles:**
  - [Chip Huyen - ML Systems Design](https://huyenchip.com/machine-learning-systems-design/toc.html)
  - [Eugene Yan - Applied ML](https://applyingml.com/)
  - [Netflix Tech Blog - ML Platform](https://netflixtechblog.com/)

### AI Project Selection:
- **Frameworks:**
  - üìñ [Andrew Ng - AI Transformation Playbook](https://landing.ai/resources/ai-transformation-playbook/) ‚≠ê
  - üìñ [Google - People + AI Guidebook](https://pair.withgoogle.com/)
  - üìñ [Microsoft - Human-AI Guidelines](https://www.microsoft.com/en-us/research/project/guidelines-for-human-ai-interaction/)

- **Articles:**
  - [a16z AI Canon](https://a16z.com/ai-canon/)
  - [Why AI Projects Fail - MIT Sloan](https://sloanreview.mit.edu/article/why-ai-projects-fail/)
  - [DataRobot - AI Project Checklist](https://www.datarobot.com/)

---

## üéì Key Concepts to Master

### Discovery:
- **Opportunity Solution Tree:** Framework connecting outcomes ‚Üí opportunities ‚Üí solutions ‚Üí experiments
- **JTBD:** Understanding the "job" users hire products to do (functional, emotional, social)
- **Forces of Progress:** Push (current pain), Pull (new solution), Anxiety (change fear), Habit (status quo)
- **Switch Interview:** Understanding moments when users change behavior
- **Discovery Mindset:** Continuous exploration vs. one-time research

### ML Project Lifecycle:
- **Problem Definition:** Framing as ML problem vs. rules-based
- **Data Requirements:** Volume, variety, velocity, veracity
- **Training vs. Inference:** Batch vs. online learning
- **Model Evaluation:** Offline metrics vs. online metrics
- **Deployment Patterns:** Shadow mode, A/B testing, canary deployment
- **Monitoring:** Drift detection, performance degradation, data quality
- **MLOps:** CI/CD for ML, model versioning, experiment tracking

### AI Project Selection:
- **Impact-Feasibility Matrix:** Prioritization for AI projects
- **Data Availability:** Most critical factor for ML success
- **Technical Debt:** Long-term costs of ML systems
- **Quick Wins vs. Moonshots:** Balancing portfolio
- **Build vs. Buy vs. Fine-tune:** Decision framework

---

## üí° Interview Questions You Should Be Able to Answer

### Discovery Questions:
1. "How do you discover and validate product opportunities?"
   - **Answer with:** OST framework + JTBD + User interviews + Example

2. "Tell me about a time you discovered an unexpected user need."
   - **STAR format:** Situation, Task, Action, Result

3. "How do you know what features to build next?"
   - **Answer with:** Discovery process + Prioritization + Validation

### AI-Specific Questions:
4. "How do you decide whether to use AI for a feature?"
   - **Answer with:** Project selection framework + Data assessment + Feasibility

5. "Walk me through how you would build a recommendation system."
   - **Answer with:** ML lifecycle phases + User needs + Success metrics

6. "What makes an AI project successful vs. unsuccessful?"
   - **Answer with:** Data quality + Clear problem + User value + Monitoring

7. "How do you work with ML engineers?"
   - **Answer with:** ML lifecycle understanding + Communication + Shared metrics

### Prioritization Questions:
8. "How do you prioritize AI features when you have limited resources?"
   - **Answer with:** Impact-feasibility matrix + RICE + Strategic alignment

---

## üéØ Tips for Success

1. **Practice frameworks on real products** - Don't just learn theory, apply it
2. **Talk to ML engineers** - If possible, shadow or interview someone building ML systems
3. **Study failures** - Learn more from what went wrong than what went right
4. **Build muscle memory** - Practice drawing OSTs and explaining JTBD fluently
5. **Connect to Week 1** - Use your opportunities from Week 1 as practice material

---

## üîó Connections to Interview Success

This week builds critical skills for:
- **Product Sense Interviews:** Discovery and opportunity identification
- **Execution Interviews:** ML project lifecycle and prioritization
- **Strategy Interviews:** Choosing which AI bets to make
- **Technical Interviews:** Understanding ML workflows and constraints

**Common Interview Scenario:**
"Design an AI feature for [product]"

**Your Approach (using this week's learning):**
1. Define outcome (what changes for users?)
2. Identify opportunities (JTBD + OST)
3. Assess feasibility (data, complexity, value)
4. Map ML lifecycle (how would we build it?)
5. Define success metrics (Week 3 focus)

---

## ‚è≠Ô∏è Coming Up in Week 3

**Focus:** Metrics + AI in Companies
- PM success metrics (AARRR, retention cohorts)
- AI-specific metrics (latency, cost per inference, drift)
- Evaluation frameworks
- Real-world AI case studies
- **Deliverable:** Metrics dashboard + 2 AI case analyses

---

## üìä Progress Tracking

| Day | Topic | Hours Completed | Status |
|-----|-------|----------------|--------|
| Mon | Opportunity Solution Trees | __ / 4 | ‚¨ú |
| Tue | JTBD Deep Dive | __ / 4 | ‚¨ú |
| Wed | Discovery Interviews | __ / 4 | ‚¨ú |
| Thu | ML Project Lifecycle | __ / 4 | ‚¨ú |
| Fri | AI Project Selection | __ / 4 | ‚¨ú |
| Sat-Sun | Apply Framework & Case Studies | __ / 5 | ‚¨ú |

**Total Target Hours:** 25 hours
**Actual Hours:** ___ hours

---

**Week 2 Success Criteria:**
‚úÖ You can explain OST and JTBD frameworks fluently
‚úÖ You can map the ML project lifecycle for any AI product
‚úÖ You have a repeatable framework for choosing AI projects
‚úÖ You can conduct a discovery interview
‚úÖ You understand the PM role in ML projects

Let's build world-class discovery skills! üöÄ
