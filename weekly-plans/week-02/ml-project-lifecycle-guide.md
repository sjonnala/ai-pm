# ML Project Lifecycle Guide for Product Managers

## üéØ Overview

Understanding the ML project lifecycle is essential for AI PMs. This guide provides a detailed walkthrough of each phase, common pitfalls, and how PMs can add value throughout the process.

**Why This Matters:** Most ML projects fail not due to technical limitations, but due to poor project management, unclear goals, or misaligned expectations. As a PM, you're the bridge between business needs and technical execution.

---

## üìö Table of Contents

1. [The 6 Phases of ML Projects](#the-6-phases-of-ml-projects)
2. [Phase 1: Problem Definition & Scoping](#phase-1-problem-definition--scoping)
3. [Phase 2: Data Collection & Preparation](#phase-2-data-collection--preparation)
4. [Phase 3: Model Development](#phase-3-model-development)
5. [Phase 4: Evaluation & Validation](#phase-4-evaluation--validation)
5. [Phase 5: Deployment](#phase-5-deployment)
6. [Phase 6: Monitoring & Maintenance](#phase-6-monitoring--maintenance)
7. [Common Failure Modes](#common-failure-modes)
8. [PM Success Checklist](#pm-success-checklist)

---

## üîÑ The 6 Phases of ML Projects

```
1. Problem Definition & Scoping (PM-Led)
        ‚Üì
2. Data Collection & Preparation (ML + Data Eng)
        ‚Üì
3. Model Development (ML Engineering)
        ‚Üì
4. Evaluation & Validation (ML + PM)
        ‚Üì
5. Deployment (ML + Software Eng)
        ‚Üì
6. Monitoring & Maintenance (Ongoing)
        ‚Üì
    [Iterate back to Phase 1 as needed]
```

**Key Insight:** These phases are iterative, not linear. You'll often loop back to earlier phases based on learnings.

---

## üìã Phase 1: Problem Definition & Scoping

### **Duration:** 1-2 weeks
### **PM Involvement:** üî•üî•üî•üî•üî• (100% - You lead this!)

### **Goals:**
- Define the business problem clearly
- Determine if ML is the right solution
- Set success criteria
- Align stakeholders

---

### **Step 1.1: Define the Business Problem**

**Framework: The 5 Ws**

**What:** What problem are we solving?
- ‚úÖ Good: "Content marketers spend 3 hours writing blog posts, causing burnout and inconsistent publishing"
- ‚ùå Bad: "We need AI for content"

**Who:** Who has this problem?
- ‚úÖ Good: "Content marketing managers at B2B SaaS companies (50-500 employees)"
- ‚ùå Bad: "People who write"

**Why:** Why does it matter?
- ‚úÖ Good: "Lost revenue from inconsistent content, employee churn from burnout, can't scale content with growth"
- ‚ùå Bad: "It would be nice to have"

**When:** When does this problem occur?
- ‚úÖ Good: "Every time they create a new blog post (8-12x per month)"
- ‚ùå Bad: "Sometimes"

**Where:** Where in the workflow?
- ‚úÖ Good: "During the drafting phase, specifically when starting from blank page"
- ‚ùå Bad: "In the content process"

---

### **Step 1.2: Determine if ML is Appropriate**

**Decision Tree:**

```
Is the problem well-defined with measurable outcomes?
    ‚îî‚îÄ No ‚Üí Go back and clarify first
    ‚îî‚îÄ Yes ‚Üì

Can simple rules solve it?
    ‚îî‚îÄ Yes ‚Üí Use rules (don't overcomplicate!)
    ‚îî‚îÄ No ‚Üì

Do you have (or can you get) sufficient data?
    ‚îî‚îÄ No ‚Üí Start with rules, collect data, add ML later
    ‚îî‚îÄ Yes ‚Üì

Is the problem pattern-recognition or prediction?
    ‚îî‚îÄ No ‚Üí ML might not be suitable
    ‚îî‚îÄ Yes ‚Üì

Can you tolerate some errors?
    ‚îî‚îÄ No ‚Üí Use deterministic approach or add heavy oversight
    ‚îî‚îÄ Yes ‚Üì

Is the ROI worth the investment?
    ‚îî‚îÄ No ‚Üí Deprioritize
    ‚îî‚îÄ Yes ‚Üí ‚úÖ Pursue ML solution
```

---

### **Step 1.3: Define Success Criteria**

**Three Types of Metrics:**

**1. Business Metrics (North Star)**
- What business outcome are we driving?
- Example: "Increase blog posts published per month from 8 ‚Üí 12"

**2. Product Metrics (User Behavior)**
- How will users interact with the ML feature?
- Example: "80% of users generate at least 1 draft per week"

**3. ML Metrics (Model Performance)**
- How accurate must the model be?
- Example: "Generated content rated 4/5+ on quality by users"

**Example - AI Writing Assistant:**

| Metric Type | Metric | Target | Timeframe |
|-------------|--------|--------|-----------|
| Business | Blog posts published/month | 8 ‚Üí 12 | 6 months |
| Product | Weekly active users | 60% of sign-ups | 3 months |
| Product | Drafts generated per user | 3/week | 3 months |
| ML | User satisfaction rating | 4+/5 | 3 months |
| ML | Acceptance rate (user publishes AI draft) | >50% | 6 months |

**PM Role:** Set these targets in collaboration with stakeholders and ML team. Be ambitious but realistic.

---

### **Step 1.4: Assess Feasibility**

**Questions to Answer:**

**Technical Feasibility:**
- Can current ML technology solve this? (research state-of-the-art)
- What's the accuracy of existing solutions?
- Example: "GPT-4 can generate coherent blog posts ‚Üí Technically feasible ‚úÖ"

**Data Feasibility:**
- What data do we need?
- Do we have it? Can we collect it?
- Example: "Need example blog posts ‚Üí Can scrape from users' published content ‚úÖ"

**Resource Feasibility:**
- Do we have ML expertise?
- What's the timeline and budget?
- Example: "6-month timeline, $200K budget, 2 ML engineers ‚Üí Feasible ‚úÖ"

**User Acceptance:**
- Will users trust ML for this task?
- What's the risk if it fails?
- Example: "Users willing to try AI drafts if they can edit ‚Üí Acceptable risk ‚úÖ"

---

### **Step 1.5: Create Project Brief**

**Template:**

```markdown
# ML Project Brief: [Project Name]

## Problem Statement
[Clear, specific description of the problem]

## Business Impact
- Current state: [metrics]
- Desired state: [metrics]
- ROI: [estimated value]

## Target Users
[Specific user segments]

## Proposed ML Solution
[High-level approach - classification, generation, recommendation, etc.]

## Success Metrics
- Business: [metrics]
- Product: [metrics]
- ML: [metrics]

## Feasibility Assessment
- Technical: [‚úÖ/‚ö†Ô∏è/‚ùå]
- Data: [‚úÖ/‚ö†Ô∏è/‚ùå]
- Resources: [‚úÖ/‚ö†Ô∏è/‚ùå]
- User acceptance: [‚úÖ/‚ö†Ô∏è/‚ùå]

## Timeline & Resources
- Duration: [X months]
- Team: [roles needed]
- Budget: [$X]

## Risks & Mitigations
[Key risks and how to address them]

## Success Criteria
[Clear definition of "done" and "successful"]
```

**Deliverable:** Get stakeholder sign-off on this brief before proceeding!

---

## üìä Phase 2: Data Collection & Preparation

### **Duration:** 4-8 weeks (often 40% of total project time!)
### **PM Involvement:** üî•üî•üî• (Medium-High - Facilitate, unblock, prioritize)

### **Goals:**
- Collect sufficient high-quality data
- Clean and prepare data for training
- Ensure data pipeline is sustainable

**PM Mindset:** Data work is often underestimated and unglamorous, but it's the foundation. Budget time and resources accordingly.

---

### **Step 2.1: Data Requirements Definition**

**Questions to Answer:**

**Quantity:**
- How much data do we need?
- Rule of thumb:
  - Simple tasks: 1K-10K examples
  - Complex tasks: 100K-1M+ examples
  - LLMs (pre-trained): Can work with 100s of examples for fine-tuning

**Quality:**
- What makes "good" data?
- Accuracy, completeness, relevance, recency

**Format:**
- What format? (text, images, tabular, etc.)
- What labels? (for supervised learning)

**Sources:**
- Where will data come from?
  - Internal: User activity logs, databases
  - External: Public datasets, purchased data, scraped data
  - Generated: User-created, crowdsourced

**Example - AI Writing Assistant:**

```
Data Needed:
- Quantity: 10K blog posts for initial training
- Quality: High-quality, published posts (not drafts)
- Format: Text (markdown or HTML)
- Labels: Category, target audience, performance metrics (traffic, engagement)
- Sources:
  - User-contributed (opt-in to share published posts)
  - Public blogs (with permission)
  - Purchased content datasets
```

---

### **Step 2.2: Data Collection Strategy**

**Approaches:**

**A. Existing Data**
- Pros: Fast, cheap
- Cons: May not be perfect fit
- PM Role: Identify what internal data exists, get access

**B. User-Generated Data**
- Pros: High relevance, continuous collection
- Cons: Slow to accumulate, privacy concerns
- PM Role: Design data collection into product, handle consent

**C. Labeling/Annotation**
- Pros: Custom labels for your use case
- Cons: Expensive, time-consuming
- PM Role: Budget for labeling, define labeling guidelines

**D. Synthetic Data**
- Pros: Can generate unlimited data
- Cons: May not reflect real-world distribution
- PM Role: Validate synthetic data quality with ML team

**E. Data Partnerships**
- Pros: Access to large datasets
- Cons: Legal complexity, cost
- PM Role: Negotiate partnerships, legal review

---

### **Step 2.3: Data Labeling (If Needed)**

**For Supervised Learning, you need labels.**

**Labeling Approaches:**

**1. Manual Labeling**
- Internal team labels data
- Pros: High quality, full control
- Cons: Slow, doesn't scale
- When to use: Small datasets, sensitive data

**2. Crowdsourced Labeling**
- Platforms: Amazon Mechanical Turk, Scale AI, Labelbox
- Pros: Fast, scalable, cost-effective
- Cons: Variable quality, requires quality control
- When to use: Large datasets, clear labeling guidelines

**3. Programmatic Labeling**
- Use rules or weak supervision
- Pros: Fast, cheap
- Cons: Noisy labels
- When to use: When rough labels are sufficient

**PM Role: Labeling Guidelines**

Create clear instructions for labelers:

```markdown
# Labeling Guide: Blog Post Quality

## Task
Rate each blog post on quality (1-5 scale)

## Rating Criteria

5 - Excellent:
- Well-structured with clear sections
- Original insights, not generic
- No grammar errors
- Engaging writing style
- SEO-optimized (headers, keywords)

4 - Good:
- Structured but could be better
- Some original insights
- Minor grammar issues
- Readable style
- Some SEO elements

3 - Average:
- Basic structure
- Mostly generic content
- Several grammar issues
- Dry writing
- Minimal SEO

2 - Below Average:
[...]

1 - Poor:
[...]

## Examples
[Provide 2-3 examples for each rating]
```

**Budget for Quality Control:**
- Multiple labelers per item (3-5 for consensus)
- Gold standard test set (measure labeler accuracy)
- Regular calibration sessions

---

### **Step 2.4: Data Cleaning & Preparation**

**Common Data Issues:**

**1. Missing Data**
- How to handle: Impute, remove, or flag

**2. Outliers**
- How to handle: Investigate (error or real?), cap, or remove

**3. Inconsistent Formatting**
- How to handle: Standardize formats, normalize

**4. Duplicate Data**
- How to handle: Deduplicate

**5. Biased Data**
- How to handle: Balance classes, collect more diverse data

**PM Role:**
- Understand what data quality issues exist
- Prioritize which issues to fix (not everything needs to be perfect)
- Make trade-offs (speed vs. quality)

---

### **Step 2.5: Data Pipeline**

**For Production ML, you need ongoing data:**

**Components:**
1. **Collection:** How data flows from sources
2. **Storage:** Where data is stored (data warehouse, data lake)
3. **Processing:** ETL (Extract, Transform, Load) jobs
4. **Versioning:** Track data versions for reproducibility
5. **Monitoring:** Detect data quality issues, drift

**PM Role:**
- Ensure data engineering is resourced
- Prioritize data pipeline robustness (it's infrastructure, not sexy, but critical)
- Plan for data refresh cadence (weekly? monthly? real-time?)

---

## üß™ Phase 3: Model Development

### **Duration:** 3-6 weeks
### **PM Involvement:** üî•üî• (Low-Medium - Set requirements, stay informed, don't micromanage)

### **Goals:**
- Build and train ML model
- Iterate to improve performance
- Prepare model for deployment

**PM Mindset:** This is the ML team's domain. Your job is to provide clear requirements, remove blockers, and understand trade-offs - not to dictate algorithms.

---

### **Step 3.1: Define Model Requirements**

**PM provides constraints and priorities:**

**Performance Requirements:**
- Minimum accuracy/quality threshold
- Example: "Content quality rated 4+/5 by users"

**Latency Requirements:**
- How fast must inference be?
- Example: "Generate 500-word draft in <30 seconds"

**Cost Requirements:**
- Budget for inference costs
- Example: "<$0.50 per draft generated"

**Fairness/Bias Requirements:**
- What biases are unacceptable?
- Example: "Must work equally well for all content topics"

**Explainability Requirements:**
- Do we need to explain predictions?
- Example: "Show which sources AI used for information"

---

### **Step 3.2: Model Selection (ML Team Decides)**

**Common Approaches:**

**For AI Writing Assistant:**
- **Option 1:** Use pre-trained LLM API (GPT-4, Claude)
  - Pros: Fast, high quality, no training
  - Cons: Expensive per request, no customization

- **Option 2:** Fine-tune open-source LLM (Llama, Mistral)
  - Pros: Customizable, lower per-request cost
  - Cons: Upfront training cost, requires ML expertise

- **Option 3:** Build custom model from scratch
  - Pros: Full control
  - Cons: Very expensive, time-consuming, requires massive data

**PM Role:** Understand trade-offs, help team choose based on priorities (speed to market, cost, quality, control)

---

### **Step 3.3: Training & Iteration**

**What Happens:**
1. ML team trains initial model
2. Evaluates on test data
3. Identifies issues (low accuracy, bias, errors)
4. Iterates (try different approaches, tune parameters)
5. Repeats until performance is acceptable

**Timeline:** This takes multiple iterations over weeks

**PM Role:**
- **Weekly check-ins:** "How's performance trending? Any blockers?"
- **Don't ask:** "Why not just make it 100% accurate?" (unrealistic)
- **Do ask:** "What's the accuracy vs. latency trade-off? What do users care more about?"
- **Unblock:** Get more data, more compute, more time if needed

---

### **Step 3.4: Understand Key Metrics**

**PM should understand these concepts:**

**For Classification Models:**
- **Accuracy:** Overall % correct
- **Precision:** Of predicted positives, % actually positive
- **Recall:** Of actual positives, % we caught
- **F1 Score:** Balance of precision and recall

**For Generative Models (LLMs):**
- **BLEU/ROUGE:** Similarity to reference text (rough proxy)
- **Perplexity:** Model confidence (lower = better)
- **Human evaluation:** User ratings (most important!)

**For Ranking/Recommendation:**
- **NDCG:** Quality of ranking
- **Click-through rate:** User engagement
- **Diversity:** Variety in recommendations

**PM Role:** Ask "What does 85% accuracy mean for users?" (not just the number)

---

## ‚úÖ Phase 4: Evaluation & Validation

### **Duration:** 2-4 weeks
### **PM Involvement:** üî•üî•üî•üî• (High - You drive this!)

### **Goals:**
- Validate model meets success criteria
- Test with real users
- De-risk before full launch

---

### **Step 4.1: Offline Evaluation**

**Test on held-out data** (data model hasn't seen)

**Metrics:**
- ML metrics (accuracy, precision, recall, etc.)
- Performance benchmarks (latency, throughput)
- Error analysis (what types of mistakes?)

**PM Role:**
- Ensure test data represents real-world usage
- Understand error patterns ("It fails on technical topics" ‚Üí need more training data)
- Decide: Is performance good enough to proceed?

---

### **Step 4.2: Online Evaluation (A/B Test)**

**Test with real users in production**

**Design:**
- Control: Existing experience (or no AI)
- Treatment: New ML model
- Split: 50/50 or 10/90 (if risky)

**Metrics:**
- Product metrics: Engagement, retention, satisfaction
- ML metrics: Acceptance rate, user ratings
- Business metrics: Posts published, revenue impact

**Duration:** 2-4 weeks minimum (ensure statistical significance)

**PM Role:**
- Design A/B test (hypothesis, metrics, duration, sample size)
- Monitor results daily
- Decide: Ship, iterate, or kill

---

### **Step 4.3: Human Evaluation**

**Have real users evaluate outputs**

**Approaches:**

**1. Internal Testing**
- Your team uses the feature
- Pros: Fast, qualitative feedback
- Cons: Not representative of real users

**2. Beta Testing**
- Select users get early access
- Pros: Real user feedback
- Cons: Takes time to recruit and gather data

**3. Crowdsourced Evaluation**
- Hire raters to evaluate outputs
- Pros: Scalable, quantitative
- Cons: Raters aren't actual users

**Evaluation Rubric Example:**

```markdown
# AI Draft Quality Evaluation

Rate the AI-generated blog draft on:

1. Relevance (1-5)
   - Does it match the topic and brief?

2. Quality (1-5)
   - Is it well-written, clear, engaging?

3. Accuracy (1-5)
   - Is information correct?

4. Usefulness (1-5)
   - Would you publish this (with minor edits)?

5. Overall (1-5)
   - Your overall rating
```

**PM Role:** Design evaluation rubric, recruit evaluators, synthesize feedback

---

### **Step 4.4: Red Teaming (AI Safety)**

**Try to break the model** - find edge cases, failures, biases

**Test for:**
- **Harmful outputs:** Offensive, biased, dangerous content
- **Jailbreaks:** Can users trick model into bad behavior?
- **Privacy leaks:** Does model expose training data?
- **Hallucinations:** Does model make up facts?
- **Brittleness:** Does it fail on slight variations?

**PM Role:** Organize red-teaming sessions, document issues, prioritize fixes

---

## üöÄ Phase 5: Deployment

### **Duration:** 2-4 weeks
### **PM Involvement:** üî•üî•üî•üî• (High - You manage the rollout)

### **Goals:**
- Ship model to production safely
- Monitor for issues
- Iterate based on real-world performance

---

### **Step 5.1: Deployment Strategy**

**Rollout Approaches:**

**1. Big Bang (0 ‚Üí 100%)**
- Ship to all users at once
- Pros: Fast
- Cons: High risk if something breaks
- When to use: Low-risk features, well-tested

**2. Gradual Rollout**
- 5% ‚Üí 10% ‚Üí 25% ‚Üí 50% ‚Üí 100% over weeks
- Pros: De-risk, catch issues early
- Cons: Slower
- When to use: High-impact features, AI products (recommended!)

**3. Feature Flag**
- Ship code, enable for specific users/cohorts
- Pros: Maximum control, easy rollback
- Cons: Added complexity
- When to use: Enterprise products, high-risk changes

**4. Canary Deployment**
- Deploy to small subset of infrastructure first
- Pros: Catch infrastructure issues
- Cons: More complex setup
- When to use: Mission-critical systems

**PM Recommendation for AI Products:**
> Start with 5-10% of users, monitor closely for 1 week, then expand gradually based on metrics.

---

### **Step 5.2: Monitoring & Alerting**

**Set up monitoring BEFORE launch!**

**What to Monitor:**

**1. Model Performance**
- Accuracy, error rate
- User satisfaction scores
- Alert: If accuracy drops >5% from baseline

**2. System Performance**
- Latency (p50, p95, p99)
- Throughput (requests per second)
- Error rate (5xx errors, timeouts)
- Alert: If latency >2x normal or error rate >1%

**3. Business Metrics**
- Feature usage (% of users engaging)
- Task completion (% successfully using feature)
- Impact on North Star (are we moving it?)
- Alert: If usage drops >20% unexpectedly

**4. Cost**
- Inference cost per request
- Total daily/monthly cost
- Alert: If cost >20% over budget

**PM Role:** Define alert thresholds, escalation paths, on-call rotation

---

### **Step 5.3: Rollback Plan**

**Always have a rollback plan!**

**If Things Go Wrong:**
1. **Immediate:** Disable feature flag (revert to pre-AI experience)
2. **Short-term:** Investigate root cause, fix issues
3. **Long-term:** Re-deploy with fixes, gradual rollout again

**PM Role:** Ensure engineering has implemented feature flags, test rollback procedure before launch

---

## üîç Phase 6: Monitoring & Maintenance

### **Duration:** Ongoing (forever!)
### **PM Involvement:** üî•üî•üî• (Medium - Ongoing responsibility)

### **Goals:**
- Keep model performing well over time
- Detect and fix degradation
- Plan for retraining

**PM Mindset:** ML products require ongoing maintenance. Budget time and resources for this!

---

### **Step 6.1: Continuous Monitoring**

**Weekly Reviews:**
- Model performance metrics (trending up/down?)
- User feedback and support tickets
- Error analysis (new failure modes?)
- Cost tracking (increasing unexpectedly?)

**Monthly Deep Dives:**
- Segment analysis (which users/use cases are struggling?)
- Competitive benchmarking (are competitors better now?)
- Roadmap planning (what improvements to prioritize?)

---

### **Step 6.2: Detecting Model Drift**

**Two Types of Drift:**

**1. Data Drift**
- Input data distribution changes
- Example: Users start asking about new topics not in training data
- Detection: Monitor input feature distributions

**2. Concept Drift**
- Relationship between inputs and outputs changes
- Example: User preferences shift over time
- Detection: Model accuracy degrades on recent data

**PM Role:**
- Understand what drift looks like for your use case
- Set up dashboards to visualize drift
- Define thresholds for when to retrain

---

### **Step 6.3: Retraining Strategy**

**When to Retrain:**
- **Time-based:** Every 3/6/12 months (scheduled)
- **Performance-based:** When accuracy drops below threshold
- **Data-based:** When you have X% more training data
- **Event-based:** Major product changes, new features

**Retraining Process:**
1. Collect new data (production data, user feedback)
2. Combine with original training data
3. Retrain model with same/improved approach
4. Evaluate on test set
5. A/B test new model vs. current
6. Deploy if better

**Cost:** Plan for ongoing ML engineering time (10-20% of team capacity)

---

### **Step 6.4: Continuous Improvement**

**Feedback Loop:**
```
User Feedback ‚Üí Identify Issues ‚Üí Prioritize Fixes ‚Üí Update Model ‚Üí Ship ‚Üí Measure ‚Üí Repeat
```

**Sources of Improvement:**
- User feedback (thumbs up/down, surveys)
- Support tickets (what are users struggling with?)
- Error analysis (what mistakes is model making?)
- Competitive intelligence (what are others doing better?)
- Technology evolution (new models, techniques)

**PM Role:**
- Maintain backlog of model improvements
- Prioritize based on impact and effort
- Plan quarterly roadmap for ML improvements

---

## ‚ö†Ô∏è Common Failure Modes

### **1. Unclear Problem Definition**
**Symptom:** Team builds solution to wrong problem
**Prevention:** Spend time on Phase 1, validate with users

### **2. Insufficient Data**
**Symptom:** Model doesn't learn, low accuracy
**Prevention:** Validate data availability early, have data collection plan

### **3. Misaligned Metrics**
**Symptom:** High ML accuracy but low user satisfaction
**Prevention:** Define success holistically (business + product + ML metrics)

### **4. Underestimating Data Prep Time**
**Symptom:** Timeline slips, data work takes 80% of project
**Prevention:** Budget 40-50% of timeline for data work

### **5. No Rollback Plan**
**Symptom:** Model breaks in production, can't revert quickly
**Prevention:** Feature flags, tested rollback procedure

### **6. Ignoring Maintenance**
**Symptom:** Model degrades over time, user complaints increase
**Prevention:** Budget for ongoing monitoring and retraining

### **7. Over-Engineering**
**Symptom:** Spend months on perfect model instead of shipping and learning
**Prevention:** Start with MVP, iterate based on real feedback

### **8. Under-Engineering**
**Symptom:** Ship too early, poor quality damages trust
**Prevention:** Set minimum quality bar, validate before full rollout

---

## ‚úÖ PM Success Checklist

### **Phase 1: Problem Definition**
- [ ] Business problem clearly defined with measurable impact
- [ ] Validated that ML is appropriate solution
- [ ] Success metrics defined (business + product + ML)
- [ ] Feasibility assessed (technical, data, resources, user)
- [ ] Stakeholders aligned on goals and timeline
- [ ] Project brief documented and approved

### **Phase 2: Data**
- [ ] Data requirements specified (quantity, quality, sources)
- [ ] Data collection strategy defined and resourced
- [ ] Labeling guidelines created (if needed)
- [ ] Budget allocated for data acquisition/labeling
- [ ] Data pipeline planned for ongoing collection
- [ ] Data quality issues identified and prioritized

### **Phase 3: Model Development**
- [ ] Model requirements communicated (performance, latency, cost)
- [ ] ML team has necessary resources (compute, data, time)
- [ ] Weekly check-ins with ML team to track progress
- [ ] Trade-offs understood and prioritized
- [ ] Blockers identified and resolved quickly

### **Phase 4: Evaluation**
- [ ] Offline evaluation completed, meets minimum bar
- [ ] A/B test designed (hypothesis, metrics, duration)
- [ ] Human evaluation conducted with real users
- [ ] Red-teaming completed, safety issues addressed
- [ ] Decision made: Ship, iterate, or kill

### **Phase 5: Deployment**
- [ ] Rollout strategy defined (gradual recommended)
- [ ] Monitoring and alerting set up
- [ ] Rollback plan tested
- [ ] Launch comms prepared (users, stakeholders)
- [ ] Post-launch review scheduled

### **Phase 6: Maintenance**
- [ ] Monitoring dashboards reviewed weekly
- [ ] Drift detection in place
- [ ] Retraining schedule defined
- [ ] Continuous improvement backlog maintained
- [ ] Resources allocated for ongoing work (10-20% capacity)

---

## üéØ Week 2 Deliverable

**Create an ML Project Lifecycle Plan for one of your AI opportunities:**

1. **Choose an opportunity** from Week 1
2. **Document each phase:**
   - Phase 1: Problem definition (use templates from this guide)
   - Phase 2: Data plan (what data, where from, how to collect)
   - Phase 3: Model approach (API vs. fine-tune vs. build)
   - Phase 4: Evaluation strategy (metrics, A/B test design)
   - Phase 5: Deployment plan (rollout strategy)
   - Phase 6: Maintenance plan (monitoring, retraining)
3. **Estimate timeline:** How long will each phase take?
4. **Identify risks:** What could go wrong in each phase?

**Output:** A 2-3 page ML project plan that shows you understand the full lifecycle.

---

## üìö Resources

**Courses:**
- üéì **Google's ML Engineering for Production (MLOps)** - Free on Coursera
- üéì **Full Stack Deep Learning** - End-to-end ML projects

**Books:**
- üìö **"Machine Learning Engineering"** by Andriy Burkov
- üìö **"Building Machine Learning Powered Applications"** by Emmanuel Ameisen

**Tools:**
- **Weights & Biases** - Experiment tracking
- **MLflow** - ML lifecycle management
- **Label Studio** - Data labeling

---

## üé¨ Conclusion

The ML project lifecycle is complex with many moving parts. As a PM, you don't need to be an ML expert, but you DO need to:

1. **Lead problem definition** (your core strength)
2. **Facilitate data work** (often the bottleneck)
3. **Understand trade-offs** (accuracy vs. latency vs. cost)
4. **Design evaluation** (ensure model meets user needs)
5. **Manage deployment** (de-risk with gradual rollouts)
6. **Plan for maintenance** (ML is never "done")

Master this lifecycle and you'll be able to ship successful AI products consistently! üöÄ

---

**Last Updated:** 2025-01-15
