# Metrics Dashboard Template for AI Products

**Created:** [Date]
**Week 3 Deliverable**
**Product:** [Your AI Product Name]

---

## Executive Summary

**North Star Metric:** [Single metric that best captures core value delivery]

**Current Value:** [e.g., "500 hours saved per week" or "10,000 DAU"]

**Target (30 days):** [Goal]

**Target (90 days):** [Goal]

**Dashboard Owner:** [Name]

**Last Updated:** [Date]

---

## 1. North Star Metric

### Definition
**Metric Name:** [e.g., "Time saved on writing tasks"]

**Formula:** [How is it calculated?]

**Why This Metric:**
[Explain why this metric best captures value delivery to users]

### Current Performance
- **Current:** [Value]
- **Last Week:** [Value] ([+/- %] change)
- **Last Month:** [Value] ([+/- %] change)

### Targets
| Timeframe | Target | Rationale |
|-----------|--------|-----------|
| 30 days | [Value] | [Why this target] |
| 60 days | [Value] | [Why this target] |
| 90 days | [Value] | [Why this target] |

---

## 2. Input Metrics (What Drives the North Star)

### Input Metric #1: [Name]
**Definition:** [What does it measure?]

**Why It Matters:** [How does this drive the North Star?]

**Current:** [Value]

**Target:** [Value]

---

### Input Metric #2: [Name]
**Definition:**

**Why It Matters:**

**Current:** [Value]

**Target:** [Value]

---

### Input Metric #3: [Name]
**Definition:**

**Why It Matters:**

**Current:** [Value]

**Target:** [Value]

---

## 3. AARRR Framework Metrics

### Acquisition
**How users discover and sign up**

| Metric | Definition | Current | Target | Status |
|--------|------------|---------|--------|--------|
| Sign-ups | New user registrations | [#] | [#] | [ğŸŸ¢/ğŸŸ¡/ğŸ”´] |
| Traffic Sources | Where users come from | [Top 3] | - | - |
| Conversion Rate | Visitor â†’ Sign-up | [%] | [%] | [ğŸŸ¢/ğŸŸ¡/ğŸ”´] |
| CAC | Cost to acquire customer | $[X] | $[X] | [ğŸŸ¢/ğŸŸ¡/ğŸ”´] |

**Insights:**
[What's working? What's not?]

---

### Activation
**First value experience**

| Metric | Definition | Current | Target | Status |
|--------|------------|---------|--------|--------|
| Activation Event | [Define: e.g., "Used AI feature 3+ times"] | [%] | [%] | [ğŸŸ¢/ğŸŸ¡/ğŸ”´] |
| Time to First Value | Time to activation event | [X min] | [X min] | [ğŸŸ¢/ğŸŸ¡/ğŸ”´] |
| Activation Rate | % of sign-ups who activate | [%] | [%] | [ğŸŸ¢/ğŸŸ¡/ğŸ”´] |
| Setup Completion | % who complete onboarding | [%] | [%] | [ğŸŸ¢/ğŸŸ¡/ğŸ”´] |

**Insights:**
[Where do users drop off? What helps activation?]

---

### Retention
**Users coming back**

| Metric | Definition | Current | Target | Status |
|--------|------------|---------|--------|--------|
| D1 Retention | % active day after sign-up | [%] | [%] | [ğŸŸ¢/ğŸŸ¡/ğŸ”´] |
| D7 Retention | % active 7 days after sign-up | [%] | [%] | [ğŸŸ¢/ğŸŸ¡/ğŸ”´] |
| D30 Retention | % active 30 days after sign-up | [%] | [%] | [ğŸŸ¢/ğŸŸ¡/ğŸ”´] |
| DAU/MAU Ratio | Stickiness metric | [%] | >20% | [ğŸŸ¢/ğŸŸ¡/ğŸ”´] |
| Weekly Active Users | Users active in past 7 days | [#] | [#] | [ğŸŸ¢/ğŸŸ¡/ğŸ”´] |

**Retention Cohort Analysis:**
```
Cohort    | D1   | D7   | D14  | D30  | D60  | D90
----------|------|------|------|------|------|------
Jan 2024  | [%]  | [%]  | [%]  | [%]  | [%]  | [%]
Feb 2024  | [%]  | [%]  | [%]  | [%]  | [%]  | -
Mar 2024  | [%]  | [%]  | [%]  | [%]  | -    | -
Apr 2024  | [%]  | [%]  | [%]  | -    | -    | -
```

**Insights:**
[Which cohorts retain better? Why? When do users churn?]

---

### Revenue
**Monetization metrics**

| Metric | Definition | Current | Target | Status |
|--------|------------|---------|--------|--------|
| Conversion to Paid | % of free users who pay | [%] | [%] | [ğŸŸ¢/ğŸŸ¡/ğŸ”´] |
| ARPU | Average revenue per user | $[X] | $[X] | [ğŸŸ¢/ğŸŸ¡/ğŸ”´] |
| MRR | Monthly recurring revenue | $[X] | $[X] | [ğŸŸ¢/ğŸŸ¡/ğŸ”´] |
| LTV | Customer lifetime value | $[X] | $[X] | [ğŸŸ¢/ğŸŸ¡/ğŸ”´] |
| LTV:CAC Ratio | Efficiency metric | [X:1] | >3:1 | [ğŸŸ¢/ğŸŸ¡/ğŸ”´] |

**Pricing Tiers:**
| Tier | Price | % of Users | ARPU |
|------|-------|------------|------|
| Free | $0 | [%] | $0 |
| Basic | $[X]/mo | [%] | $[X] |
| Pro | $[X]/mo | [%] | $[X] |
| Enterprise | $[X]/mo | [%] | $[X] |

**Insights:**
[Which tiers convert best? Price sensitivity? Upgrade patterns?]

---

### Referral
**Viral growth**

| Metric | Definition | Current | Target | Status |
|--------|------------|---------|--------|--------|
| Invite Rate | % of users who send invites | [%] | [%] | [ğŸŸ¢/ğŸŸ¡/ğŸ”´] |
| Viral Coefficient | New users per existing user | [X] | >1 | [ğŸŸ¢/ğŸŸ¡/ğŸ”´] |
| Referral Conversion | % of invites that sign up | [%] | [%] | [ğŸŸ¢/ğŸŸ¡/ğŸ”´] |
| NPS | Net Promoter Score | [X] | >40 | [ğŸŸ¢/ğŸŸ¡/ğŸ”´] |

**Insights:**
[What drives referrals? Who are power inviters?]

---

## 4. AI-Specific Metrics

### Model Performance Metrics

| Metric | Definition | Current | Target | Status |
|--------|------------|---------|--------|--------|
| Accuracy | [Task-specific: e.g., classification accuracy] | [%] | [%] | [ğŸŸ¢/ğŸŸ¡/ğŸ”´] |
| Precision | True positives / (True pos + False pos) | [%] | [%] | [ğŸŸ¢/ğŸŸ¡/ğŸ”´] |
| Recall | True positives / (True pos + False neg) | [%] | [%] | [ğŸŸ¢/ğŸŸ¡/ğŸ”´] |
| F1 Score | Harmonic mean of precision & recall | [X] | [X] | [ğŸŸ¢/ğŸŸ¡/ğŸ”´] |
| [Custom Metric] | [e.g., BLEU score for translation] | [X] | [X] | [ğŸŸ¢/ğŸŸ¡/ğŸ”´] |

**Insights:**
[Are we meeting quality thresholds? Trends?]

---

### User Satisfaction with AI

| Metric | Definition | Current | Target | Status |
|--------|------------|---------|--------|--------|
| Acceptance Rate | % of AI suggestions accepted | [%] | [%] | [ğŸŸ¢/ğŸŸ¡/ğŸ”´] |
| Thumbs Up Rate | % of positive feedback | [%] | >80% | [ğŸŸ¢/ğŸŸ¡/ğŸ”´] |
| Edit Rate | % of outputs edited by user | [%] | <30% | [ğŸŸ¢/ğŸŸ¡/ğŸ”´] |
| Re-generation Rate | % of times user asks for new output | [%] | <20% | [ğŸŸ¢/ğŸŸ¡/ğŸ”´] |
| Task Completion | % of tasks successfully completed | [%] | [%] | [ğŸŸ¢/ğŸŸ¡/ğŸ”´] |

**User Feedback Themes:**
- **Most Common Praise:** [Theme]
- **Most Common Complaint:** [Theme]
- **Feature Requests:** [Top 3]

---

### Operational Metrics

| Metric | Definition | Current | Target | Status |
|--------|------------|---------|--------|--------|
| Latency (p50) | 50th percentile response time | [X ms] | <500ms | [ğŸŸ¢/ğŸŸ¡/ğŸ”´] |
| Latency (p95) | 95th percentile response time | [X ms] | <2000ms | [ğŸŸ¢/ğŸŸ¡/ğŸ”´] |
| Latency (p99) | 99th percentile response time | [X ms] | <5000ms | [ğŸŸ¢/ğŸŸ¡/ğŸ”´] |
| Throughput | Requests per second | [X] | [X] | [ğŸŸ¢/ğŸŸ¡/ğŸ”´] |
| Error Rate | % of failed requests | [%] | <1% | [ğŸŸ¢/ğŸŸ¡/ğŸ”´] |
| Uptime | % availability | [%] | >99.9% | [ğŸŸ¢/ğŸŸ¡/ğŸ”´] |

**Insights:**
[Performance bottlenecks? SLA compliance?]

---

### Cost Metrics

| Metric | Definition | Current | Target | Status |
|--------|------------|---------|--------|--------|
| Cost per Inference | Average cost per AI request | $[X] | $[X] | [ğŸŸ¢/ğŸŸ¡/ğŸ”´] |
| Cost per User (Monthly) | AI costs / monthly active users | $[X] | $[X] | [ğŸŸ¢/ğŸŸ¡/ğŸ”´] |
| Total AI Spend | Monthly AI infrastructure costs | $[X] | $[X] | [ğŸŸ¢/ğŸŸ¡/ğŸ”´] |
| Margin | Revenue - AI costs / Revenue | [%] | >70% | [ğŸŸ¢/ğŸŸ¡/ğŸ”´] |

**Cost Breakdown:**
- Model inference: $[X] ([%])
- Data storage: $[X] ([%])
- Embeddings/vectors: $[X] ([%])
- Other: $[X] ([%])

**Insights:**
[Cost efficiency trends? Opportunities to optimize?]

---

### Data Quality & Drift

| Metric | Definition | Current | Alert Threshold | Status |
|--------|------------|---------|-----------------|--------|
| Data Completeness | % of expected data present | [%] | <95% | [ğŸŸ¢/ğŸŸ¡/ğŸ”´] |
| Data Freshness | Time since last update | [X hrs] | >24hr | [ğŸŸ¢/ğŸŸ¡/ğŸ”´] |
| Input Distribution Shift | KS statistic vs. baseline | [X] | >0.1 | [ğŸŸ¢/ğŸŸ¡/ğŸ”´] |
| Prediction Drift | Output distribution change | [X] | >15% | [ğŸŸ¢/ğŸŸ¡/ğŸ”´] |
| Label Accuracy | Human-verified label quality | [%] | <98% | [ğŸŸ¢/ğŸŸ¡/ğŸ”´] |

**Insights:**
[Any drift detected? Data quality issues?]

---

## 5. Guardrail Metrics (Counter Metrics)

**These prevent gaming the primary metrics**

| Metric | Definition | Current | Alert Threshold | Status |
|--------|------------|---------|-----------------|--------|
| Churn Rate | % of users who stop using | [%] | >5%/mo | [ğŸŸ¢/ğŸŸ¡/ğŸ”´] |
| Safety Incidents | Harmful/inappropriate outputs | [#] | >0 | [ğŸŸ¢/ğŸŸ¡/ğŸ”´] |
| User Complaints | Support tickets about quality | [#/week] | >10 | [ğŸŸ¢/ğŸŸ¡/ğŸ”´] |
| Time to Value | How long to get value | [X min] | >10min | [ğŸŸ¢/ğŸŸ¡/ğŸ”´] |
| Feature Bloat | # of features per user flow | [X] | >5 | [ğŸŸ¢/ğŸŸ¡/ğŸ”´] |

**Why These Matter:**
[Explain how these prevent unintended consequences]

---

## 6. Metrics Hierarchy & Relationships

```
                    NORTH STAR METRIC
                    [Your North Star]
                           |
         __________________|__________________
        |                  |                  |
   Input Metric 1    Input Metric 2    Input Metric 3
        |                  |                  |
   [Sub-metrics]      [Sub-metrics]      [Sub-metrics]
```

**How Metrics Connect:**
- [Input Metric 1] drives [North Star] by [mechanism]
- [Input Metric 2] drives [North Star] by [mechanism]
- [Input Metric 3] drives [North Star] by [mechanism]

---

## 7. Weekly Tracking

### Week of [Date]

**ğŸ¯ Weekly Goals:**
1. [Goal 1 with metric target]
2. [Goal 2 with metric target]
3. [Goal 3 with metric target]

**ğŸ“Š Results:**
| Metric | Target | Actual | Status |
|--------|--------|--------|--------|
| [Metric 1] | [Value] | [Value] | [ğŸŸ¢/ğŸŸ¡/ğŸ”´] |
| [Metric 2] | [Value] | [Value] | [ğŸŸ¢/ğŸŸ¡/ğŸ”´] |
| [Metric 3] | [Value] | [Value] | [ğŸŸ¢/ğŸŸ¡/ğŸ”´] |

**ğŸ“ˆ What Worked:**
- [Success 1]
- [Success 2]

**ğŸ“‰ What Didn't:**
- [Challenge 1]
- [Challenge 2]

**ğŸ”„ Next Week Focus:**
- [Action 1]
- [Action 2]

---

## 8. Experiment Tracker

**Current Experiments:**

### Experiment #1: [Name]
**Hypothesis:** If we [change], then [metric] will improve by [%] because [reason]

**Status:** [Planning / Running / Completed]

**Variant:** [Control vs. Treatment description]

**Success Metric:** [Primary metric]

**Guardrail Metrics:** [What we're watching]

**Sample Size:** [# users needed]

**Duration:** [Start date] - [End date]

**Results:** [To be filled when complete]

---

### Experiment #2: [Name]
[Repeat structure]

---

## 9. Dashboard Insights & Actions

### Key Insights This Period
1. **[Insight 1]:** [What we learned]
   - **Action:** [What we'll do]

2. **[Insight 2]:** [What we learned]
   - **Action:** [What we'll do]

3. **[Insight 3]:** [What we learned]
   - **Action:** [What we'll do]

### Red Flags / Concerns
- âš ï¸ [Concern 1 with metric]
- âš ï¸ [Concern 2 with metric]

### Opportunities
- ğŸ’¡ [Opportunity 1]
- ğŸ’¡ [Opportunity 2]

---

## 10. Visual Dashboard Mockup

**[Insert screenshot, Figma link, or ASCII visualization]**

Example visualization:
```
NORTH STAR: Time Saved
Current: 500 hrs/week | Target: 1000 hrs/week
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ 50%

ACQUISITION              ACTIVATION
Sign-ups: 1,250 â†‘15%    Rate: 65% â†‘5%
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”      â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸŸ¢ On track              ğŸŸ¢ On track

RETENTION               AI QUALITY
D7: 45% â†“3%            Acceptance: 72%
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”      â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸŸ¡ Needs attention      ğŸŸ¢ On track
```

---

## 11. Metric Definitions Glossary

**For easy reference:**

- **DAU:** Daily Active Users - users who performed [key action] in past 24 hours
- **MAU:** Monthly Active Users - users who performed [key action] in past 30 days
- **Retention:** % of cohort still active after X days
- **Activation:** User completed [specific action] that indicates value received
- **Churn:** User hasn't been active for [X days]
- **[Custom term]:** [Your definition]

---

## Interview Application

**When asked: "How would you measure success for [your AI product]?"**

Use this dashboard to answer:

"I'd use a layered approach:

**North Star:** [Your North Star metric] - this captures core value delivery

**Product Metrics:** AARRR framework tracking acquisition through referral

**AI Metrics:** Model performance (accuracy, latency), user satisfaction (acceptance rate), and costs

**Guardrails:** Safety incidents, churn, user complaints

I'd track these daily/weekly and run experiments to improve the North Star by optimizing input metrics while maintaining guardrails."

**Practice explaining your metrics strategy in 3 minutes!**
