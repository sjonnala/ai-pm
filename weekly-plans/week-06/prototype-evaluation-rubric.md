# AI Prototype Evaluation Rubric

**Created:** [Date]
**Week 6 Deliverable**
**Product:** [Your AI Product Name]
**Self-Assessment Date:** [Date]

---

## How to Use This Rubric

**Purpose:** Objectively evaluate your AI prototype quality across 6 dimensions

**Scoring:**
- **1 - Needs Work:** Below acceptable standard
- **2 - Acceptable:** Meets minimum requirement
- **3 - Good:** Above average, solid work
- **4 - Excellent:** Exceptional, portfolio-worthy
- **5 - Outstanding:** Could ship to real users

**Target for Week 6:** Average score of **3.0+** (Good)

**Portfolio-Ready:** Average score of **3.5+** (Between Good and Excellent)

---

## Evaluation Dimensions

### 1. Core Functionality

Does the prototype successfully demonstrate the core AI capability?

| Score | Criteria |
|-------|----------|
| **1** | Core feature doesn't work reliably; frequent failures |
| **2** | Core feature works but with significant limitations or inconsistency |
| **3** | Core feature works reliably for happy path; some edge case issues |
| **4** | Core feature works well across multiple scenarios; handles most edge cases |
| **5** | Core feature is robust, handles edge cases gracefully, delights users |

**Your Score:** [ ] / 5

**Evidence:**
- Success rate in testing: [X]%
- User feedback: [Summary]
- Key limitations: [List]

**What would move you to the next level:**
[Specific improvements needed]

---

### 2. AI Output Quality

How good are the AI-generated outputs?

| Score | Criteria |
|-------|----------|
| **1** | Outputs are frequently irrelevant, inaccurate, or unhelpful |
| **2** | Outputs are sometimes useful but often need significant editing (>50% of time) |
| **3** | Outputs are usually helpful and require minor edits (20-50% of time) |
| **4** | Outputs are consistently high-quality, rarely need editing (<20% of time) |
| **5** | Outputs are exceptional, better than what users could create themselves |

**Your Score:** [ ] / 5

**Evidence:**
- Acceptance rate: [X]%
- Edit rate: [X]%
- User satisfaction with quality: [X]/10
- Sample outputs quality: [Assessment]

**Metrics:**
- Relevance: [1-10]
- Accuracy: [1-10]
- Completeness: [1-10]
- Creativity/Uniqueness: [1-10]

**What would move you to the next level:**
[Specific prompt engineering or model improvements needed]

---

### 3. User Experience & Design

How intuitive and polished is the user interface?

| Score | Criteria |
|-------|----------|
| **1** | Confusing interface; users don't know what to do; looks broken |
| **2** | Functional but clunky; requires instructions; minimal polish |
| **3** | Intuitive for most users; clear value prop; reasonable polish |
| **4** | Smooth UX; clear guidance; good visual design; feels professional |
| **5** | Delightful UX; anticipates user needs; beautiful design; memorable |

**Your Score:** [ ] / 5

**Evidence:**
- First-time users completed task without help: [X]/[Y]
- Time to first successful use: [X] seconds
- User comments on UX: [Quotes]

**UX Elements Checklist:**
- [ ] Clear value proposition on landing
- [ ] Obvious call-to-action
- [ ] Input area is prominent and clear
- [ ] Loading states are informative
- [ ] Outputs are well-formatted
- [ ] Error messages are helpful
- [ ] Mobile-friendly (if applicable)
- [ ] Visually appealing

**What would move you to the next level:**
[Specific UX improvements]

---

### 4. Prompt Engineering & AI Implementation

How well-designed are the prompts and AI implementation?

| Score | Criteria |
|-------|----------|
| **1** | Basic prompts with no optimization; no consideration of parameters |
| **2** | Simple prompts with minimal iteration; basic parameter usage |
| **3** | Well-structured prompts; 3+ iterations; appropriate parameter tuning |
| **4** | Sophisticated prompts; clear versioning; advanced techniques (few-shot, CoT) |
| **5** | Expert-level prompts; systematic optimization; multiple advanced techniques |

**Your Score:** [ ] / 5

**Evidence:**
- Number of prompt iterations: [X]
- Techniques used: [List - e.g., few-shot, chain-of-thought, etc.]
- Parameter optimization: [Temperature, max tokens, etc.]
- A/B testing conducted: [Yes/No]

**Prompt Engineering Checklist:**
- [ ] System prompt is well-structured
- [ ] Clear role and constraints defined
- [ ] Output format is specified
- [ ] Examples provided (few-shot learning)
- [ ] Edge cases are handled in prompt
- [ ] Temperature is optimized for use case
- [ ] Token length is appropriate
- [ ] Cost is considered in design
- [ ] Versioning system in place

**What would move you to the next level:**
[Specific prompt improvements]

---

### 5. Error Handling & Edge Cases

How well does it handle unexpected inputs and failures?

| Score | Criteria |
|-------|----------|
| **1** | Breaks or gives unhelpful errors with unexpected input |
| **2** | Shows generic error messages; doesn't guide user to success |
| **3** | Handles common edge cases; provides helpful error messages |
| **4** | Gracefully handles most edge cases; offers alternatives when failing |
| **5** | Anticipates edge cases; turns errors into learning moments; always helpful |

**Your Score:** [ ] / 5

**Evidence:**
- Edge cases tested: [X]
- Handled successfully: [Y]
- Error message quality: [Assessment]

**Edge Cases Tested:**
- [ ] Empty input
- [ ] Very short input (1-2 words)
- [ ] Very long input (500+ words)
- [ ] Vague/ambiguous input
- [ ] Input in unexpected format
- [ ] Nonsensical input
- [ ] Input outside domain
- [ ] Requests violating guidelines
- [ ] Multiple languages (if applicable)
- [ ] Special characters

**Error Handling Checklist:**
- [ ] No blank error screens
- [ ] Errors explain what went wrong
- [ ] Errors suggest what to do next
- [ ] Users can easily retry
- [ ] Graceful degradation (doesn't crash)
- [ ] Fallback options provided
- [ ] Maintains user input (doesn't lose work)

**What would move you to the next level:**
[Specific error handling improvements]

---

### 6. User Testing & Iteration

How thoroughly have you tested and iterated based on feedback?

| Score | Criteria |
|-------|----------|
| **1** | No external testing; only self-tested |
| **2** | Tested with 1-2 people; minimal iteration based on feedback |
| **3** | Tested with 5+ users; iterated based on findings; documented learnings |
| **4** | Tested with 10+ users; multiple iterations; clear before/after metrics |
| **5** | Rigorous testing protocol; quantitative and qualitative data; systematic iteration |

**Your Score:** [ ] / 5

**Evidence:**
- Number of external testers: [X]
- Rounds of iteration: [Y]
- Key changes made: [List]
- Metrics improvement: [Before â†’ After]

**Testing Checklist:**
- [ ] Tested with target users (not just friends)
- [ ] Observed actual usage (not just asked opinions)
- [ ] Collected quantitative feedback (ratings)
- [ ] Collected qualitative feedback (quotes)
- [ ] Identified patterns in feedback
- [ ] Prioritized changes based on data
- [ ] Re-tested after iterations
- [ ] Documented all findings

**User Testing Summary:**
- Users tested: [X]
- Average satisfaction: [Y]/10
- Task completion rate: [Z]%
- Key insight 1: [Insight]
- Key insight 2: [Insight]
- Key insight 3: [Insight]

**What would move you to the next level:**
[More testing or specific iterations needed]

---

## Overall Assessment

### Score Summary

| Dimension | Score | Weight | Weighted Score |
|-----------|-------|--------|----------------|
| 1. Core Functionality | [ ]/5 | 25% | [ ] |
| 2. AI Output Quality | [ ]/5 | 25% | [ ] |
| 3. User Experience | [ ]/5 | 15% | [ ] |
| 4. Prompt Engineering | [ ]/5 | 15% | [ ] |
| 5. Error Handling | [ ]/5 | 10% | [ ] |
| 6. User Testing | [ ]/5 | 10% | [ ] |
| **Total** | | **100%** | **[ ] / 5.0** |

---

### Overall Rating

**[ ] / 5.0**

**Interpretation:**
- **4.0+ : Outstanding** - Portfolio showcase piece
- **3.5-3.9 : Excellent** - Strong portfolio addition
- **3.0-3.4 : Good** - Meets Week 6 goals
- **2.5-2.9 : Acceptable** - Needs minor improvements
- **<2.5 : Needs Work** - Significant improvements needed

**Your Status:** [Outstanding / Excellent / Good / Acceptable / Needs Work]

---

### Strengths (Top 3)

1. [Strength 1 - what you did really well]
2. [Strength 2]
3. [Strength 3]

---

### Areas for Improvement (Top 3 Priority)

1. **[Improvement Area 1]**
   - Current state: [What's the issue]
   - Target state: [What would be better]
   - Action: [Specific next step]
   - Estimated effort: [Hours/Days]
   - Impact: [High/Medium/Low]

2. **[Improvement Area 2]**
   - Current state:
   - Target state:
   - Action:
   - Estimated effort:
   - Impact:

3. **[Improvement Area 3]**
   - Current state:
   - Target state:
   - Action:
   - Estimated effort:
   - Impact:

---

## Portfolio Readiness

### Is This Portfolio-Ready? (Score 3.5+)

**[ ] Yes - Ready to showcase**
**[ ] Almost - Minor polish needed**
**[ ] Not Yet - Significant work needed**

### Portfolio Checklist

If using in portfolio, ensure you have:

**Prototype:**
- [ ] Live, accessible URL
- [ ] Works reliably when accessed
- [ ] Looks professional
- [ ] Has sample data/examples preloaded

**Documentation:**
- [ ] Clear description of what it does
- [ ] Problem it solves explained
- [ ] Key features highlighted
- [ ] Technical approach described
- [ ] Learnings documented

**Visuals:**
- [ ] Demo video (2-3 minutes)
- [ ] Screenshots with annotations
- [ ] User flow diagrams
- [ ] Metrics/results displayed

**Evidence:**
- [ ] User testing results
- [ ] Satisfaction metrics
- [ ] Before/after comparisons
- [ ] User quotes/testimonials

---

## Interview Readiness

### Can You Answer These Questions?

**About the Product:**
- [ ] "Walk me through your prototype" (2 min version)
- [ ] "What problem does this solve?" (30 sec)
- [ ] "Who is this for and why?" (30 sec)
- [ ] "What was your biggest challenge?" (1 min)
- [ ] "How did you validate this idea?" (1 min)

**About the AI:**
- [ ] "What AI model/approach did you use and why?" (1 min)
- [ ] "How did you optimize the prompts?" (1 min)
- [ ] "What did you learn about prompt engineering?" (1 min)
- [ ] "How does it handle errors or edge cases?" (1 min)
- [ ] "What would you do differently?" (1 min)

**About the Process:**
- [ ] "How did you test this?" (1 min)
- [ ] "What did user testing reveal?" (1 min)
- [ ] "How did you prioritize features?" (1 min)
- [ ] "What metrics did you track?" (1 min)
- [ ] "What would V2 include?" (1 min)

**About Impact:**
- [ ] "What was the outcome?" (30 sec)
- [ ] "How did you measure success?" (1 min)
- [ ] "What business value does this create?" (1 min)

**Readiness Score:** [ ] / 15 questions answered confidently

**Action:** Practice answering the questions you couldn't answer well.

---

## Comparison to Real Products

### How Does Your Prototype Compare?

**Choose a comparable real product:** [e.g., ChatGPT, Grammarly, Midjourney, etc.]

| Dimension | Real Product | Your Prototype | Gap Analysis |
|-----------|--------------|----------------|--------------|
| Core Functionality | [Rating] | [Rating] | [What's the gap?] |
| Output Quality | [Rating] | [Rating] | [What's the gap?] |
| User Experience | [Rating] | [Rating] | [What's the gap?] |
| Error Handling | [Rating] | [Rating] | [What's the gap?] |

**Key Insight:** [What did this comparison teach you?]

**Realistic Expectations:**
- Your prototype is a 1-week MVP
- Real products have teams of 10-100+ people
- Real products have months/years of iteration
- Your goal: Demonstrate capability, not match production quality

---

## Red Flags (Fix Immediately)

Check if any of these apply:

**Critical Issues:**
- [ ] Prototype doesn't work >50% of the time
- [ ] Output quality is consistently poor
- [ ] Users can't figure out how to use it
- [ ] No external user testing conducted
- [ ] Contains offensive/unsafe outputs
- [ ] API keys are exposed in code
- [ ] Takes >10 seconds to respond
- [ ] Cost per query is unsustainable (>$1/query)

**If any are checked:** Stop and fix before sharing publicly.

---

## Green Lights (You're Ready!)

Celebrate if you have:

**Achievements:**
- [ ] Prototype works reliably (>80% success)
- [ ] Users rated it >7/10 satisfaction
- [ ] You tested with 5+ external users
- [ ] You iterated based on feedback
- [ ] You can demo it in <3 minutes
- [ ] You have concrete metrics/results
- [ ] You learned 3+ specific lessons
- [ ] You're proud to share it

**If 6+ checked:** You're ready to showcase this!

---

## Final Decision

### What's Next?

**Option A: Ship It**
- Score is 3.0+
- No critical red flags
- Portfolio-ready
- **Action:** Create demo video, add to portfolio, share on LinkedIn

**Option B: Polish First**
- Score is 2.5-2.9
- Minor improvements needed
- Close to ready
- **Action:** Spend 1-2 more days on top priority improvements, then ship

**Option C: Iterate More**
- Score is <2.5
- Significant gaps
- Not ready for portfolio
- **Action:** Focus on top 2 improvement areas, re-test, re-evaluate

**Your Choice:** [A / B / C]

**Rationale:** [Why you chose this]

---

## Improvement Roadmap

### If You Choose to Iterate

**Week 6 (This Week):**
- [ ] [Priority improvement 1]
- [ ] [Priority improvement 2]
- [ ] Re-test with 3-5 users

**Week 7 (If Continuing):**
- [ ] [Feature addition or enhancement]
- [ ] [UX polish]
- [ ] Final round of testing

**Week 8+ (Future):**
- [ ] [Advanced feature]
- [ ] [Scale/performance improvements]
- [ ] [Monetization considerations]

---

## Accountability

### Commitment

**I commit to:**
- [ ] Being honest in this self-assessment
- [ ] Testing with real external users (not just friends)
- [ ] Iterating based on feedback, not assumptions
- [ ] Shipping something, even if imperfect
- [ ] Learning from the process

**Target Completion Date:** [Date]

**Accountability Partner:** [Name, if applicable]

**Check-in Schedule:** [When you'll review progress]

---

## Reflection Questions

### Learning Assessment

**What did building this teach you about AI products?**
[Your reflection]

**What surprised you most?**
[Your reflection]

**What was harder than expected?**
[Your reflection]

**What was easier than expected?**
[Your reflection]

**If you started over, what would you do differently?**
[Your reflection]

**How has this prepared you for AI PM interviews?**
[Your reflection]

**What's the #1 lesson you'll take forward?**
[Your reflection]

---

## Peer Review (Optional)

### Get Feedback from Another PM

**Reviewer:** [Name]
**Date:** [Date]

**Their Overall Score:** [ ] / 5.0

**Their Top Strength:** [What they said]

**Their Top Improvement Suggestion:** [What they said]

**Surprising Feedback:** [What you didn't expect]

**Action Based on Feedback:** [What you'll change]

---

## Final Checklist

Before moving to Week 7:

**Deliverables Complete:**
- [ ] Working prototype (live URL)
- [ ] Prompt engineering playbook
- [ ] 5+ UX flows documented
- [ ] User testing results captured
- [ ] Demo video created (optional but recommended)
- [ ] Prototype evaluation completed (this doc)

**Quality Bar Met:**
- [ ] Overall score â‰¥ 3.0
- [ ] No critical red flags
- [ ] External user testing done
- [ ] Can confidently demo in interviews

**Portfolio Ready:**
- [ ] Added to portfolio website
- [ ] Shared on LinkedIn
- [ ] Demo video uploaded
- [ ] Metrics and learnings documented

**Learning Objectives Achieved:**
- [ ] Can rapidly prototype with AI
- [ ] Understand prompt engineering
- [ ] Can test and iterate on AI products
- [ ] Have concrete portfolio piece
- [ ] Ready to discuss in interviews

---

**Congratulations on building your AI prototype! ðŸŽ‰**

**Remember:** The goal wasn't perfection. The goal was learning by doing. You now have:
1. A concrete portfolio piece
2. Hands-on AI product experience
3. Interview stories to tell
4. Foundation for Week 7-12

**Keep going! You're building real AI PM skills. ðŸš€**
