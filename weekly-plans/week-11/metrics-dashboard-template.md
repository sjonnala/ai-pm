# AI Product Metrics Dashboard

**Created:** [Date]
**Week 11 Portfolio Project Deliverable**
**Product Name:** [Your AI Product Name]
**Prepared by:** [Your Name]
**Version:** [e.g., 1.0]

---

## Executive Summary

**Dashboard Purpose:**

[In 2-3 sentences, explain what this metrics dashboard tracks and why these metrics matter]

Example: "This dashboard tracks the health and performance of our AI documentation assistant across product engagement, AI quality, and business outcomes. It enables data-driven decisions and early detection of issues."

**Your Dashboard Purpose:**

[Your turn]

---

**Metrics at a Glance:**

| Category | Key Metric | Current Value | Target | Status |
|----------|------------|---------------|--------|--------|
| **North Star** | [Your north star] | [Current] | [Target] | [ðŸŸ¢/ðŸŸ¡/ðŸ”´] |
| **Product** | [Key product metric] | [Current] | [Target] | [Status] |
| **AI Quality** | [Key AI metric] | [Current] | [Target] | [Status] |
| **Business** | [Key business metric] | [Current] | [Target] | [Status] |
| **Operations** | [Key ops metric] | [Current] | [Target] | [Status] |

---

## Table of Contents

1. [Dashboard Structure & Philosophy](#part-1-dashboard-structure--philosophy)
2. [North Star Metric](#part-2-north-star-metric)
3. [Product Metrics (AARRR)](#part-3-product-metrics-aarrr)
4. [AI/ML Performance Metrics](#part-4-aiml-performance-metrics)
5. [User Experience Metrics](#part-5-user-experience-metrics)
6. [Business & Revenue Metrics](#part-6-business--revenue-metrics)
7. [Operational Metrics](#part-7-operational-metrics)
8. [Guardrail Metrics](#part-8-guardrail-metrics)
9. [Dashboard Visualization](#part-9-dashboard-visualization)
10. [Reporting & Review Cadence](#part-10-reporting--review-cadence)

---

## Part 1: Dashboard Structure & Philosophy

### 1.1 Metrics Philosophy

**Guiding Principles:**

[What guides your metrics choices?]

Example principles:
- **Actionable:** Every metric should inform a decision
- **Comprehensive:** Cover product, AI, and business health
- **Simple:** Easy to understand at a glance
- **Leading & Lagging:** Mix of predictive and outcome metrics
- **Honest:** Show what matters, even if it's not flattering

**Your Principles:**

1. [Principle 1]
2. [Principle 2]
3. [Principle 3]
4. [Principle 4]
5. [Principle 5]

---

### 1.2 Dashboard Hierarchy

**Level 1: Executive View**
- **Audience:** Leadership, investors, stakeholders
- **Metrics:** North Star + 3-5 key metrics
- **Update Frequency:** Weekly
- **Format:** One-page summary

**Level 2: Product View**
- **Audience:** Product team, AI PM
- **Metrics:** AARRR funnel + engagement + retention
- **Update Frequency:** Daily
- **Format:** Interactive dashboard

**Level 3: AI/ML View**
- **Audience:** ML engineers, AI PM, QA
- **Metrics:** Model performance, quality, latency, cost
- **Update Frequency:** Real-time + daily summaries
- **Format:** Technical dashboard

**Level 4: Operational View**
- **Audience:** Engineering, DevOps
- **Metrics:** Infrastructure, errors, costs, uptime
- **Update Frequency:** Real-time
- **Format:** Monitoring dashboards

---

### 1.3 Metric Categories

**How Metrics Are Organized:**

```
North Star Metric (Top-level success)
    |
    â”œâ”€â”€ Product Metrics (User behavior)
    â”‚   â”œâ”€â”€ Acquisition
    â”‚   â”œâ”€â”€ Activation
    â”‚   â”œâ”€â”€ Retention
    â”‚   â”œâ”€â”€ Revenue
    â”‚   â””â”€â”€ Referral
    â”‚
    â”œâ”€â”€ AI/ML Metrics (Model performance)
    â”‚   â”œâ”€â”€ Quality (accuracy, relevance, etc.)
    â”‚   â”œâ”€â”€ Performance (latency, throughput)
    â”‚   â””â”€â”€ Cost (per inference, total)
    â”‚
    â”œâ”€â”€ Business Metrics (Outcomes)
    â”‚   â”œâ”€â”€ Growth
    â”‚   â”œâ”€â”€ Revenue
    â”‚   â””â”€â”€ Unit Economics
    â”‚
    â””â”€â”€ Guardrails (What must NOT happen)
        â”œâ”€â”€ Safety incidents
        â”œâ”€â”€ Cost overruns
        â””â”€â”€ Quality degradation
```

---

## Part 2: North Star Metric

### 2.1 Defining Your North Star

**What is a North Star Metric?**

The single metric that best captures the core value you deliver to customers. It should:
- Reflect value delivered to customers
- Predict long-term success
- Be measurable
- Be actionable

---

**Your North Star Metric:**

| Element | Your Answer |
|---------|-------------|
| **Metric Name** | [e.g., "Weekly Documentation Pages Generated"] |
| **Definition** | [Exactly how is it calculated?] |
| **Why This Metric** | [Why does this best represent value?] |
| **Current Value** | [e.g., 1,500 pages/week] |
| **30-Day Target** | [e.g., 3,000 pages/week] |
| **90-Day Target** | [e.g., 10,000 pages/week] |

---

### 2.2 Input Metrics

**What Drives the North Star?**

Identify 3-5 input metrics that, when improved, drive your North Star:

| Input Metric | Relationship to North Star | Current | Target | Priority |
|--------------|----------------------------|---------|--------|----------|
| [e.g., "Active Users"] | [More users â†’ more pages] | [500] | [1,000] | [High] |
| [e.g., "Avg Pages per User"] | [Higher usage â†’ more pages] | [3/week] | [5/week] | [High] |
| [e.g., "Activation Rate"] | [More activated users â†’ more usage] | [60%] | [75%] | [Medium] |
| [Input metric 4] | [Relationship] | [Current] | [Target] | [Priority] |

---

### 2.3 North Star Tracking

**Trend Over Time:**

[Create a line graph showing North Star over the past 4-12 weeks, plus projection]

| Week | North Star Value | Week-over-Week Growth | Notes |
|------|------------------|-----------------------|-------|
| Week 1 | [Value] | [N/A] | [Any context] |
| Week 2 | [Value] | [+X%] | [Context] |
| Week 3 | [Value] | [+X%] | [Context] |
| Week 4 | [Value] | [+X%] | [Context] |
| ... | ... | ... | ... |

**Target Growth Rate:** [e.g., "20% week-over-week for first 8 weeks"]

---

## Part 3: Product Metrics (AARRR)

### 3.1 Acquisition

**How many users are discovering your product?**

| Metric | Definition | Current | Target | Measurement Method |
|--------|------------|---------|--------|--------------------|
| **Website Visitors** | [Unique visitors per week/month] | [e.g., 5,000/mo] | [e.g., 20,000/mo] | [Google Analytics] |
| **Signups** | [New user registrations per week/month] | [e.g., 200/mo] | [e.g., 1,000/mo] | [Product analytics] |
| **Signup Rate** | [Signups / Visitors] | [e.g., 4%] | [e.g., 8%] | [Calculated] |
| **Traffic Sources** | [Top 3 sources with %] | [e.g., 40% organic, 30% social, 20% paid] | [Target mix] | [UTM tracking] |

**Acquisition Funnel:**

| Stage | Volume | Conversion Rate | Drop-off | Priority to Improve |
|-------|--------|-----------------|----------|---------------------|
| **Visitors** | [5,000] | [100%] | [â€”] | [â€”] |
| **Sign-up Page** | [500] | [10%] | [90%] | [ðŸ”´ High] |
| **Started Signup** | [300] | [60%] | [40%] | [ðŸŸ¡ Medium] |
| **Completed Signup** | [200] | [67%] | [33%] | [ðŸŸ¡ Medium] |

---

### 3.2 Activation

**How many users experience core value?**

**Define Your "Aha Moment":**

[e.g., "User generates their first AI-powered document"]

| Metric | Definition | Current | Target | Measurement |
|--------|------------|---------|--------|-------------|
| **Activation Rate** | [% of signups who reach aha moment] | [e.g., 60%] | [e.g., 80%] | [Product analytics] |
| **Time to Activation** | [Median time from signup to aha moment] | [e.g., 8 min] | [e.g., 3 min] | [Product analytics] |
| **Day 1 Retention** | [% who return on Day 1] | [e.g., 50%] | [e.g., 65%] | [Cohort analysis] |

**Activation Funnel:**

| Step | Users | Completion Rate | Drop-off | Issue |
|------|-------|-----------------|----------|-------|
| 1. [Sign up] | [200] | [100%] | [â€”] | [â€”] |
| 2. [Complete onboarding] | [150] | [75%] | [25%] | [Onboarding too long?] |
| 3. [First interaction] | [120] | [80%] | [20%] | [Unclear next step?] |
| 4. [Reach aha moment] | [100] | [83%] | [17%] | [UX friction?] |

**Overall Activation: 50%** (100/200)

---

### 3.3 Retention

**How many users come back?**

| Metric | Definition | Current | Target | Benchmark |
|--------|------------|---------|--------|-----------|
| **Day 1 Retention** | [% who return on Day 1] | [e.g., 50%] | [e.g., 65%] | [Typical: 40-60%] |
| **Day 7 Retention** | [% who return by Day 7] | [e.g., 35%] | [e.g., 50%] | [Typical: 30-50%] |
| **Day 30 Retention** | [% who return by Day 30] | [e.g., 25%] | [e.g., 40%] | [Typical: 20-40%] |
| **WAU/MAU Ratio** | [Weekly Active / Monthly Active] | [e.g., 0.4] | [e.g., 0.6] | [Good: >0.5] |

**Retention Curve:**

[Create a visualization showing cohort retention over 12 weeks]

| Cohort | Week 0 | Week 1 | Week 2 | Week 3 | Week 4 | ... | Week 12 |
|--------|--------|--------|--------|--------|--------|-----|---------|
| Jan W1 | 100% | 50% | 40% | 35% | 30% | ... | 25% |
| Jan W2 | 100% | 55% | 42% | 38% | 32% | ... | TBD |
| ... | ... | ... | ... | ... | ... | ... | ... |

**Target:** Flatten retention curve at 40% by Week 4

---

**Engagement by Cohort:**

| User Cohort | Definition | % of Users | Avg Sessions/Week | Value |
|-------------|------------|------------|-------------------|-------|
| **Power Users** | [Use 5+ times/week] | [10%] | [8] | [Highest] |
| **Regular Users** | [Use 2-4 times/week] | [30%] | [3] | [High] |
| **Casual Users** | [Use 1 time/week] | [40%] | [1] | [Medium] |
| **Dormant Users** | [Haven't used in 2+ weeks] | [20%] | [0] | [At-risk] |

**Goal:** Move casual users to regular, prevent dormant

---

### 3.4 Revenue

**How are you monetizing?**

| Metric | Definition | Current | Target | Notes |
|--------|------------|---------|--------|-------|
| **Total Users** | [All registered users] | [e.g., 1,000] | [e.g., 10,000] | [Growth] |
| **Paying Users** | [Users on paid plans] | [e.g., 50] | [e.g., 500] | [Conversion] |
| **Conversion Rate** | [Paying / Total] | [e.g., 5%] | [e.g., 10%] | [Monetization efficiency] |
| **MRR** | [Monthly Recurring Revenue] | [e.g., $2,000] | [e.g., $20,000] | [Revenue] |
| **ARPU** | [Avg Revenue Per User (all)] | [e.g., $2] | [e.g., $4] | [Per-user value] |
| **ARPPU** | [Avg Revenue Per Paying User] | [e.g., $40] | [e.g., $50] | [Paying user value] |

**Revenue by Tier:**

| Tier | Users | % of Paying | MRR per User | Total MRR | % of Revenue |
|------|-------|-------------|--------------|-----------|--------------|
| Free | [950] | [â€”] | $0 | $0 | 0% |
| Pro | [40] | [80%] | $20 | $800 | 40% |
| Team | [8] | [16%] | $100 | $800 | 40% |
| Enterprise | [2] | [4%] | $200 | $400 | 20% |
| **Total** | **1,000** | **50** | **â€”** | **$2,000** | **100%** |

---

### 3.5 Referral

**How are users helping you grow?**

| Metric | Definition | Current | Target | Notes |
|--------|------------|---------|--------|-------|
| **Referral Rate** | [% of users who refer someone] | [e.g., 10%] | [e.g., 25%] | [Viral coefficient] |
| **Invites Sent** | [Total invites sent] | [e.g., 200] | [e.g., 1,000] | [Top of funnel] |
| **Invite Acceptance** | [% of invites accepted] | [e.g., 30%] | [e.g., 40%] | [Quality of referrals] |
| **Referred Signups** | [Signups from referrals] | [e.g., 60] | [e.g., 400] | [Referral volume] |
| **K-Factor** | [Viral coefficient] | [e.g., 0.3] | [e.g., 0.6] | [Each user brings 0.3 more] |

**Referral Sources:**

| Source | Referrals | % of Total |
|--------|-----------|------------|
| [e.g., Direct invite link] | [40] | [67%] |
| [e.g., Social share] | [15] | [25%] |
| [e.g., Embedded content] | [5] | [8%] |

---

## Part 4: AI/ML Performance Metrics

### 4.1 Model Quality Metrics

**How good is your AI?**

**For LLM-based / Generative AI Products:**

| Metric | Definition | Current | Target | Measurement Method |
|--------|------------|---------|--------|--------------------|
| **Relevance Score** | [How relevant are responses? 1-5 scale] | [e.g., 4.2/5] | [e.g., 4.5/5] | [Human eval or LLM-as-judge] |
| **Accuracy Score** | [How accurate? 1-5 scale] | [e.g., 4.0/5] | [e.g., 4.3/5] | [Human eval] |
| **Helpfulness Score** | [How helpful? 1-5 scale] | [e.g., 4.1/5] | [e.g., 4.4/5] | [User ratings] |
| **Hallucination Rate** | [% of responses with factual errors] | [e.g., 5%] | [e.g., <2%] | [Human eval on sample] |
| **Task Success Rate** | [% of user tasks completed successfully] | [e.g., 80%] | [e.g., 90%] | [User testing + analytics] |

---

**For Classification / Prediction Tasks:**

| Metric | Definition | Current | Target | Measurement |
|--------|------------|---------|--------|-------------|
| **Precision** | [% of positive predictions that are correct] | [e.g., 85%] | [e.g., 90%] | [Offline eval] |
| **Recall** | [% of actual positives correctly identified] | [e.g., 80%] | [e.g., 85%] | [Offline eval] |
| **F1 Score** | [Harmonic mean of precision and recall] | [e.g., 0.82] | [e.g., 0.87] | [Calculated] |
| **Accuracy** | [% of all predictions that are correct] | [e.g., 88%] | [e.g., 92%] | [Offline eval] |
| **AUC-ROC** | [Area under ROC curve] | [e.g., 0.90] | [e.g., 0.93] | [Offline eval] |

---

### 4.2 User Satisfaction with AI

**What do users think?**

| Metric | Definition | Current | Target | Collection Method |
|--------|------------|---------|--------|-------------------|
| **Thumbs Up Rate** | [% of responses rated thumbs up] | [e.g., 75%] | [e.g., 85%] | [In-product feedback] |
| **Average Star Rating** | [Average rating (1-5 stars)] | [e.g., 4.0/5] | [e.g., 4.3/5] | [Post-interaction survey] |
| **Feedback Volume** | [# of ratings collected per week] | [e.g., 300] | [e.g., 1,000] | [Analytics] |
| **Negative Feedback Rate** | [% of interactions with thumbs down or <3 stars] | [e.g., 15%] | [e.g., <10%] | [Analytics] |

**Satisfaction Trend:**

| Week | Thumbs Up Rate | Average Rating | Sample Size | Notes |
|------|----------------|----------------|-------------|-------|
| Week 1 | [70%] | [3.8/5] | [100 ratings] | [Baseline] |
| Week 2 | [73%] | [4.0/5] | [200 ratings] | [Improved prompts] |
| Week 3 | [75%] | [4.1/5] | [300 ratings] | [Continuing upward] |
| ... | ... | ... | ... | ... |

---

### 4.3 AI Performance Metrics

**How fast and reliable is your AI?**

| Metric | Definition | Current | Target | Acceptable Range |
|--------|------------|---------|--------|------------------|
| **Latency (p50)** | [Median response time] | [e.g., 1.2s] | [e.g., <1s] | [<2s] |
| **Latency (p95)** | [95th percentile response time] | [e.g., 3.5s] | [e.g., <3s] | [<5s] |
| **Latency (p99)** | [99th percentile response time] | [e.g., 6s] | [e.g., <5s] | [<10s] |
| **Throughput** | [Requests processed per second] | [e.g., 10 req/s] | [e.g., 50 req/s] | [Scale to demand] |
| **Error Rate** | [% of requests that error] | [e.g., 0.5%] | [e.g., <0.2%] | [<1%] |
| **Timeout Rate** | [% of requests that timeout] | [e.g., 0.3%] | [e.g., <0.1%] | [<0.5%] |
| **Availability** | [% uptime] | [e.g., 99.5%] | [e.g., 99.9%] | [>99%] |

**Performance Trend:**

[Track latency over time, especially around launches or traffic spikes]

| Date | p50 Latency | p95 Latency | Error Rate | Traffic Volume |
|------|-------------|-------------|------------|----------------|
| [Date 1] | [1.2s] | [3.5s] | [0.5%] | [1000 req/day] |
| [Date 2] | [1.3s] | [4.0s] | [0.8%] | [2000 req/day] |
| ... | ... | ... | ... | ... |

---

### 4.4 AI Cost Metrics

**How much does your AI cost?**

| Metric | Definition | Current | Target | Notes |
|--------|------------|---------|--------|-------|
| **Cost per Query** | [Average AI cost per request] | [e.g., $0.03] | [e.g., <$0.02] | [LLM + infra costs] |
| **Daily AI Spend** | [Total AI costs per day] | [e.g., $100] | [e.g., Track trend] | [Monitor for spikes] |
| **Monthly AI Spend** | [Total AI costs per month] | [e.g., $3,000] | [e.g., <$5,000] | [Budget threshold] |
| **Cost as % of Revenue** | [AI costs / Revenue] | [e.g., 150%] | [e.g., <50% eventually] | [Path to profitability] |
| **Cost per Active User** | [AI costs / Active users] | [e.g., $5/user/mo] | [e.g., <$3/user/mo] | [Unit economics] |

**Cost Breakdown:**

| Cost Component | Monthly Cost | % of Total | Notes |
|----------------|--------------|------------|-------|
| **LLM API (e.g., OpenAI)** | [e.g., $2,000] | [67%] | [GPT-4 queries] |
| **Vector Database** | [e.g., $300] | [10%] | [Pinecone] |
| **Compute/Hosting** | [e.g., $500] | [17%] | [AWS] |
| **Other** | [e.g., $200] | [6%] | [Misc] |
| **Total** | **$3,000** | **100%** | [â€”] |

**Cost Optimization Opportunities:**

- [Opportunity 1: e.g., "Switch to GPT-3.5 for simple queries â†’ save 90%"]
- [Opportunity 2: e.g., "Cache common responses â†’ save 20%"]
- [Opportunity 3: e.g., "Batch processing â†’ save 10%"]

---

## Part 5: User Experience Metrics

### 5.1 Usability Metrics

| Metric | Definition | Current | Target | Measurement |
|--------|------------|---------|--------|-------------|
| **Time to First Value** | [Time from signup to aha moment] | [e.g., 8 min] | [e.g., 3 min] | [Product analytics] |
| **Task Completion Rate** | [% of started tasks completed] | [e.g., 75%] | [e.g., 85%] | [Analytics + user testing] |
| **Feature Discovery** | [% of users who find key features] | [e.g., 60%] | [e.g., 80%] | [Feature usage analytics] |
| **Support Ticket Rate** | [Tickets per 100 active users] | [e.g., 5] | [e.g., <3] | [Support system] |

---

### 5.2 Net Promoter Score (NPS)

**How likely are users to recommend your product?**

| Metric | Current | Target | Benchmark |
|--------|---------|--------|-----------|
| **NPS Score** | [e.g., 35] | [e.g., >40] | [Good: >30, Great: >50] |
| **Promoters (9-10)** | [e.g., 45%] | [e.g., >50%] | [â€”] |
| **Passives (7-8)** | [e.g., 35%] | [e.g., <30%] | [â€”] |
| **Detractors (0-6)** | [e.g., 20%] | [e.g., <15%] | [â€”] |

**NPS = % Promoters - % Detractors = 45% - 20% = 25**

**NPS by User Segment:**

| Segment | NPS | Sample Size | Insight |
|---------|-----|-------------|---------|
| **Free users** | [20] | [100] | [Lower satisfaction, expected] |
| **Paid users** | [50] | [30] | [Much higher, good sign] |
| **Power users** | [60] | [20] | [Very happy] |
| **New users (<1 week)** | [15] | [50] | [Need better onboarding] |

---

## Part 6: Business & Revenue Metrics

### 6.1 Growth Metrics

| Metric | Definition | Current | Target | Growth Rate |
|--------|------------|---------|--------|-------------|
| **Total Users** | [All registered users] | [e.g., 1,000] | [e.g., 10,000] | [+20% WoW] |
| **Active Users (MAU)** | [Monthly Active Users] | [e.g., 500] | [e.g., 5,000] | [+25% MoM] |
| **New Users (Weekly)** | [New signups per week] | [e.g., 50] | [e.g., 500] | [+20% WoW] |
| **User Growth Rate** | [Week-over-week growth %] | [e.g., 15%] | [e.g., 20%] | [Accelerating] |

**Growth Trend:**

| Week | Total Users | New Users | Growth % | MAU | Notes |
|------|-------------|-----------|----------|-----|-------|
| W1 | 500 | 50 | â€” | 250 | [Baseline] |
| W2 | 575 | 75 | +15% | 300 | [Good growth] |
| W3 | 675 | 100 | +17% | 375 | [Accelerating] |
| ... | ... | ... | ... | ... | ... |

---

### 6.2 Revenue Metrics

| Metric | Definition | Current | Target | Growth |
|--------|------------|---------|--------|--------|
| **MRR** | [Monthly Recurring Revenue] | [e.g., $2,000] | [e.g., $20,000] | [+30% MoM] |
| **ARR** | [Annual Recurring Revenue (MRR Ã— 12)] | [e.g., $24K] | [e.g., $240K] | [â€”] |
| **Revenue Growth Rate** | [Month-over-month MRR growth] | [e.g., 25%] | [e.g., 30%] | [Target] |
| **Paying Customers** | [# of paying users] | [e.g., 50] | [e.g., 500] | [+35% MoM] |

**Revenue Composition:**

| Revenue Type | Monthly | % of Total | Growth Rate |
|--------------|---------|------------|-------------|
| **New MRR** | [e.g., $800] | [40%] | [New customers] |
| **Expansion MRR** | [e.g., $200] | [10%] | [Upsells, upgrades] |
| **Churned MRR** | [e.g., -$100] | [-5%] | [Lost customers] |
| **Net New MRR** | [e.g., $900] | [45%] | [Net growth] |
| **Existing MRR** | [e.g., $1,100] | [55%] | [Retained revenue] |
| **Total MRR** | **$2,000** | **100%** | [â€”] |

---

### 6.3 Unit Economics

| Metric | Value | Calculation | Target |
|--------|-------|-------------|--------|
| **ARPU** | [e.g., $4/user/mo] | [MRR / Total Users] | [e.g., $6/user/mo] |
| **ARPPU** | [e.g., $40/user/mo] | [MRR / Paying Users] | [e.g., $50/user/mo] |
| **CAC** | [e.g., $50] | [Sales & Marketing / New Customers] | [e.g., <$40] |
| **LTV** | [e.g., $600] | [ARPPU Ã— Avg Lifetime (months)] | [e.g., $800] |
| **LTV:CAC Ratio** | [e.g., 12:1] | [LTV / CAC] | [e.g., >3:1 â€” this is great!] |
| **CAC Payback** | [e.g., 1.25 months] | [CAC / (ARPPU - COGS)] | [e.g., <6 months] |
| **Gross Margin** | [e.g., 75%] | [(Revenue - COGS) / Revenue] | [e.g., >70%] |

---

### 6.4 Churn & Retention (Revenue)

| Metric | Definition | Current | Target | Industry Benchmark |
|--------|------------|---------|--------|---------------------|
| **Gross MRR Churn** | [MRR lost from cancellations] | [e.g., 5%/mo] | [e.g., <3%/mo] | [SaaS: 3-5%/mo] |
| **Net MRR Churn** | [Gross churn - expansion] | [e.g., 3%/mo] | [e.g., <0% (negative churn)] | [Best: negative] |
| **Customer Churn** | [% of customers who cancel] | [e.g., 4%/mo] | [e.g., <3%/mo] | [SaaS: 3-7%/mo] |
| **Revenue Retention** | [% of revenue retained after 12 mo] | [e.g., TBD] | [e.g., >100%] | [Good: >90%, Great: >100%] |

**Churn Analysis:**

| Customer Segment | Churn Rate | Reason for Churn | Action to Reduce |
|------------------|------------|------------------|------------------|
| **Free â†’ Paid (trial end)** | [70%] | [Didn't see value] | [Better onboarding, value demonstration] |
| **Paid (month 1-3)** | [10%] | [Not using enough] | [Engagement campaigns, feature education] |
| **Paid (month 4+)** | [3%] | [Budget, found alternative] | [Competitive differentiation, lock-in features] |

---

## Part 7: Operational Metrics

### 7.1 Infrastructure & Reliability

| Metric | Definition | Current | Target | Alert Threshold |
|--------|------------|---------|--------|-----------------|
| **Uptime** | [% availability] | [e.g., 99.5%] | [e.g., 99.9%] | [<99%] |
| **API Response Time** | [p95 API latency] | [e.g., 200ms] | [e.g., <150ms] | [>500ms] |
| **Error Rate** | [% of requests with errors] | [e.g., 0.3%] | [e.g., <0.1%] | [>1%] |
| **Database Query Time** | [p95 DB latency] | [e.g., 50ms] | [e.g., <30ms] | [>100ms] |

---

### 7.2 Cost & Efficiency

| Metric | Definition | Current | Target | Notes |
|--------|------------|---------|--------|-------|
| **Hosting Costs** | [Monthly infrastructure cost] | [e.g., $500] | [e.g., Track] | [AWS/GCP] |
| **Cost per User** | [Total costs / Active Users] | [e.g., $10/user] | [e.g., <$5/user] | [Improve with scale] |
| **Burn Rate** | [Monthly cash spent] | [e.g., $10K] | [e.g., Track runway] | [Pre-revenue] |
| **Runway** | [Months until out of money] | [e.g., 12 months] | [e.g., >6 months] | [Fundraising trigger] |

---

## Part 8: Guardrail Metrics

**Metrics that must NOT cross thresholds â€” these protect your business**

### 8.1 Quality Guardrails

| Guardrail | Threshold | Current | Status | Action if Violated |
|-----------|-----------|---------|--------|-------------------|
| **Hallucination Rate** | [<5%] | [e.g., 3%] | [ðŸŸ¢ OK] | [Investigate, add human review] |
| **User Satisfaction** | [>70% thumbs up] | [e.g., 75%] | [ðŸŸ¢ OK] | [Pause releases, investigate] |
| **Error Rate** | [<1%] | [e.g., 0.5%] | [ðŸŸ¢ OK] | [Page on-call, rollback] |
| **Latency p95** | [<5s] | [e.g., 3.5s] | [ðŸŸ¢ OK] | [Scale infrastructure] |

---

### 8.2 Safety Guardrails

| Guardrail | Threshold | Current | Status | Action if Violated |
|-----------|-----------|---------|--------|-------------------|
| **Safety Incidents** | [0 critical incidents/week] | [0] | [ðŸŸ¢ OK] | [Immediate investigation, fix] |
| **Inappropriate Content** | [<0.1% flagged content] | [e.g., 0.05%] | [ðŸŸ¢ OK] | [Review filters, tighten] |
| **User Reports** | [<5 serious complaints/week] | [e.g., 2] | [ðŸŸ¢ OK] | [Investigate each, find patterns] |

---

### 8.3 Business Guardrails

| Guardrail | Threshold | Current | Status | Action if Violated |
|-----------|-----------|---------|--------|-------------------|
| **AI Cost % of Revenue** | [<100% by month 6] | [e.g., 150%] | [ðŸŸ¡ Monitor] | [Optimize costs, increase prices] |
| **Churn Rate** | [<5%/month] | [e.g., 4%] | [ðŸŸ¢ OK] | [Retention initiatives] |
| **CAC Payback** | [<12 months] | [e.g., 1.25 months] | [ðŸŸ¢ OK] | [Reduce CAC or increase ARPPU] |

---

## Part 9: Dashboard Visualization

### 9.1 Executive Dashboard (One-Page)

**Create a visual one-page dashboard with:**

**Section 1: North Star (Top, Large)**
- North Star metric with trend (last 4 weeks)
- Week-over-week growth %
- Progress toward target

**Section 2: Key Metrics (3-4 Cards)**
- Total Users + growth
- MRR + growth
- AI Quality Score
- User Satisfaction (NPS or thumbs up %)

**Section 3: AARRR Funnel (Visual)**
- Acquisition â†’ Activation â†’ Retention â†’ Revenue â†’ Referral
- Show conversion rates between stages

**Section 4: Alerts & Issues (Bottom)**
- Any red/yellow guardrails
- Top 3 issues to address

---

### 9.2 Detailed Dashboard Layout

**Design a multi-tab dashboard:**

**Tab 1: Overview**
- North Star + key metrics
- Growth trends (users, revenue)
- Top alerts

**Tab 2: Product Metrics**
- AARRR funnel
- Retention curves
- Engagement by cohort

**Tab 3: AI Performance**
- Quality metrics (accuracy, satisfaction)
- Performance metrics (latency, throughput)
- Cost metrics

**Tab 4: Business Metrics**
- Revenue breakdown
- Unit economics
- Churn analysis

**Tab 5: Operations**
- Infrastructure health
- Error rates
- Cost tracking

---

### 9.3 Sample Dashboard Mockup

**Create a visual mockup using:**
- **Figma** (design tool)
- **Excalidraw** (simple diagrams)
- **Google Sheets** (spreadsheet dashboard)
- **Notion** (structured dashboard)
- **Grafana/Datadog** (if technical)

**Include:**
- Line charts (trends over time)
- Bar charts (comparisons)
- Funnels (conversion flows)
- Gauges (progress toward targets)
- Tables (detailed breakdowns)
- Traffic lights (red/yellow/green status)

---

## Part 10: Reporting & Review Cadence

### 10.1 Metrics Review Schedule

**Daily:**
- Real-time operational metrics (errors, latency, uptime)
- User signups and activations
- AI performance and cost

**Weekly:**
- North Star metric progress
- AARRR metrics
- Key initiatives and experiments
- Team review meeting

**Monthly:**
- Business metrics (MRR, growth, unit economics)
- Retention and churn deep-dive
- AI quality audit
- Board/investor update (if applicable)

**Quarterly:**
- Strategic review (goals, roadmap)
- Comprehensive quality audit
- User research and insights
- Competitive analysis update

---

### 10.2 Reporting Templates

**Weekly Snapshot Email:**

```
Subject: Weekly Metrics - [Date]

North Star: [Value] ([+/- %] WoW)
Users: [Total] ([+X] new this week)
MRR: $[Value] ([+/- %] WoW)
AI Quality: [Score]/5
NPS: [Score]

ðŸŸ¢ Wins:
- [Win 1]
- [Win 2]

ðŸŸ¡ Concerns:
- [Concern 1]
- [Concern 2]

ðŸŽ¯ Focus This Week:
- [Priority 1]
- [Priority 2]
```

---

**Monthly Business Review:**

**Slide 1: Executive Summary**
- North Star achievement vs. target
- Key wins and challenges

**Slide 2: Growth**
- User growth
- Revenue growth
- Retention trends

**Slide 3: Product Health**
- AARRR metrics
- Feature adoption

**Slide 4: AI Quality**
- Performance metrics
- User satisfaction
- Cost efficiency

**Slide 5: Focus for Next Month**
- Top priorities
- Experiments to run

---

### 10.3 Metric Ownership

**Who Owns Which Metrics?**

| Metric Category | Owner | Review Cadence |
|----------------|-------|----------------|
| **North Star** | [AI PM / Product Lead] | [Daily] |
| **Product Metrics** | [AI PM] | [Daily/Weekly] |
| **AI Quality** | [AI PM + ML Engineer] | [Daily/Weekly] |
| **Revenue** | [AI PM / Founder] | [Weekly/Monthly] |
| **Operations** | [Engineering Lead] | [Real-time/Daily] |
| **User Experience** | [AI PM / Designer] | [Weekly/Monthly] |

---

## Appendix A: Metrics Glossary

**Define each metric clearly:**

| Metric | Definition | Formula | Example |
|--------|------------|---------|---------|
| **MAU** | Monthly Active Users | [Users who performed key action in last 30 days] | [User who generated 1+ doc in last 30 days] |
| **WAU** | Weekly Active Users | [Users who performed key action in last 7 days] | [User who generated 1+ doc in last 7 days] |
| **MRR** | Monthly Recurring Revenue | [Sum of all monthly subscription revenue] | [50 users Ã— $40/mo = $2,000] |
| **CAC** | Customer Acquisition Cost | [Sales & Marketing Spend / New Customers] | [$5,000 / 100 = $50/customer] |
| **LTV** | Lifetime Value | [ARPPU Ã— Average Customer Lifetime (months)] | [$40/mo Ã— 15 months = $600] |
| ... | ... | ... | ... |

---

## Appendix B: Tools & Implementation

**Tools You Might Use:**

| Purpose | Tools | Notes |
|---------|-------|-------|
| **Product Analytics** | [Amplitude, Mixpanel, PostHog, Heap] | [Track user behavior] |
| **AI Monitoring** | [LangSmith, PromptLayer, Weights & Biases] | [LLM performance] |
| **Infrastructure** | [Datadog, Grafana, CloudWatch, New Relic] | [System health] |
| **Business Metrics** | [Stripe, ChartMogul, Baremetrics] | [Revenue analytics] |
| **Dashboards** | [Notion, Google Sheets, Looker, Tableau] | [Reporting] |
| **User Feedback** | [Delighted, Survicate, Typeform] | [NPS, surveys] |

---

## Reflection & Learnings

**What I Learned About Metrics:**

[Reflect on what you learned while building this metrics dashboard. What metrics were hardest to define? What surprised you? What would you prioritize differently?]

**How This Dashboard Demonstrates AI PM Skills:**

[Explain how this metrics framework showcases your ability to measure success, balance multiple dimensions (product, AI, business), and make data-driven decisions]

**Interview Talking Points:**

[List 3-5 key points you'd emphasize when discussing this metrics dashboard in interviews]

1. [Point 1]
2. [Point 2]
3. [Point 3]
4. [Point 4]
5. [Point 5]

---

**Document Version History:**

| Version | Date | Changes | Author |
|---------|------|---------|--------|
| 1.0 | [Date] | [Initial creation] | [Your name] |

---

**End of Metrics Dashboard Template**

*Metrics evolve as your product matures. Revisit and refine this dashboard regularly.*
